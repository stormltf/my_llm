"""
è¾¹ç•Œæƒ…å†µæµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
1. ç©ºè¾“å…¥å¤„ç†
2. è¶…é•¿åºåˆ—
3. æžç«¯å‚æ•°å€¼
4. å†…å­˜ä¸è¶³æƒ…å†µ
5. æ•°å€¼æº¢å‡º
6. ç‰¹æ®Šå­—ç¬¦å¤„ç†
"""

import pytest
import torch
import torch.nn as nn

from model import GPT, GPTConfig
from tokenizer import BPETokenizer
from generate import TextGenerator


class TestEmptyInputs:
    """ç©ºè¾“å…¥æµ‹è¯•"""

    def test_empty_text_encoding(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬ç¼–ç """
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        tokens = tokenizer.encode("")

        # ç©ºå­—ç¬¦ä¸²åº”è¯¥è¿”å›žç©ºåˆ—è¡¨
        assert tokens == []

    def test_empty_prompt_generation(self):
        """æµ‹è¯•ç©ºæç¤ºç”Ÿæˆ"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # ç©ºæç¤ºåº”è¯¥è¿”å›žç©ºæˆ–æŠ¥é”™ï¼ˆå–å†³äºŽå®žçŽ°ï¼‰
        # å½“å‰å®žçŽ°ä¼šæŠ¥ IndexErrorï¼Œè¿™æ˜¯é¢„æœŸçš„
        with pytest.raises((IndexError, RuntimeError)):
            generator.generate("", max_length=10)

    def test_empty_dataset(self):
        """æµ‹è¯•ç©ºæ•°æ®é›†"""
        from train import PretrainDataset

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        dataset = PretrainDataset([], tokenizer, seq_len=16)

        assert len(dataset) == 0

    def test_batch_with_all_padding(self):
        """æµ‹è¯•éœ€è¦å¡«å……çš„æ‰¹æ¬¡"""
        from train import collate_fn

        # åˆ›å»ºé•¿åº¦ä¸åŒçš„æ ·æœ¬ï¼Œè¾ƒçŸ­çš„éœ€è¦å¡«å……åˆ°æœ€é•¿çš„é•¿åº¦
        batch = [
            (torch.tensor([1]), torch.tensor([2])),
            (torch.tensor([3, 4]), torch.tensor([4, 5])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        # åº”è¯¥æ­£ç¡®å¡«å……åˆ°æœ€é•¿åºåˆ—é•¿åº¦
        assert padded_inputs.shape == (2, 2)
        assert padded_targets.shape == (2, 2)


class TestLongSequences:
    """è¶…é•¿åºåˆ—æµ‹è¯•"""

    def test_sequence_exceeds_context_size(self):
        """æµ‹è¯•è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦çš„åºåˆ—"""
        config = GPTConfig(
            vocab_size=100,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,  # ä¸Šä¸‹æ–‡é•¿åº¦ 32
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        # åˆ›å»ºè¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦çš„è¾“å…¥
        input_ids = torch.randint(0, 100, (1, 50))  # é•¿åº¦ 50 > 32

        # åº”è¯¥æŠ›å‡ºæ–­è¨€é”™è¯¯
        with pytest.raises(AssertionError):
            model(input_ids)

    def test_sequence_equals_context_size(self):
        """æµ‹è¯•ç­‰äºŽä¸Šä¸‹æ–‡é•¿åº¦çš„åºåˆ—"""
        config = GPTConfig(
            vocab_size=100,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        # ç²¾ç¡®ç­‰äºŽä¸Šä¸‹æ–‡é•¿åº¦
        input_ids = torch.randint(0, 100, (1, 32))

        logits, _ = model(input_ids)

        # åº”è¯¥æ­£å¸¸å·¥ä½œ
        assert logits.shape == (1, 32, 100)

    def test_very_long_text_encoding(self):
        """æµ‹è¯•è¶…é•¿æ–‡æœ¬ç¼–ç """
        tokenizer = BPETokenizer(vocab_size=100)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        # åˆ›å»ºè¶…é•¿æ–‡æœ¬
        long_text = "æµ‹ è¯• " * 1000

        tokens = tokenizer.encode(long_text)

        # åº”è¯¥è¿”å›žå¤§é‡ token
        assert len(tokens) > 0

    def test_generation_with_long_prompt(self):
        """æµ‹è¯•é•¿æç¤ºç”Ÿæˆ"""
        config = GPTConfig(
            vocab_size=100,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=64,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=100)
        long_text = "æµ‹ è¯• " * 100
        tokenizer.fit([long_text], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # é•¿æç¤ºåº”è¯¥è¢«æˆªæ–­
        result = generator.generate(long_text, max_length=80)

        # åº”è¯¥è¿”å›žç»“æžœ
        assert isinstance(result, str)


class TestExtremeParameters:
    """æžç«¯å‚æ•°å€¼æµ‹è¯•"""

    def test_very_small_temperature(self):
        """æµ‹è¯•éžå¸¸å°çš„æ¸©åº¦"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # æžå°æ¸©åº¦æŽ¥è¿‘è´ªå©ªè§£ç 
        result = generator.generate("æµ‹", max_length=5, temperature=0.001)

        assert isinstance(result, str)

    def test_very_large_temperature(self):
        """æµ‹è¯•éžå¸¸å¤§çš„æ¸©åº¦"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # æžå¤§æ¸©åº¦æŽ¥è¿‘å‡åŒ€åˆ†å¸ƒ
        result = generator.generate("æµ‹", max_length=5, temperature=100.0)

        assert isinstance(result, str)

    def test_top_k_extreme_values(self):
        """æµ‹è¯• Top-k æžç«¯å€¼"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # Top-k = 1 ç­‰ä»·äºŽè´ªå©ª
        result1 = generator.generate("æµ‹", max_length=5, top_k=1)
        assert isinstance(result1, str)

        # Top-k = vocab_size ç­‰ä»·äºŽä¸ä½¿ç”¨
        result2 = generator.generate("æµ‹", max_length=5, top_k=50)
        assert isinstance(result2, str)

    def test_top_p_extreme_values(self):
        """æµ‹è¯• Top-p æžç«¯å€¼"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        generator = TextGenerator(model, tokenizer, torch.device("cpu"))

        # Top-p = 1.0 ç­‰ä»·äºŽä¸ä½¿ç”¨
        result = generator.generate("æµ‹", max_length=5, top_p=1.0)
        assert isinstance(result, str)

        # Top-p = 0.0 åº”è¯¥è‡³å°‘é€‰æ‹©ä¸€ä¸ª
        result2 = generator.generate("æµ‹", max_length=5, top_p=0.01)
        assert isinstance(result2, str)


class TestNumericalEdgeCases:
    """æ•°å€¼è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def test_zero_variance_input(self):
        """æµ‹è¯•é›¶æ–¹å·®è¾“å…¥"""
        from model import LayerNorm

        ln = LayerNorm(emb_dim=64)

        # å…¨ç›¸åŒçš„è¾“å…¥ï¼ˆæ–¹å·®ä¸º0ï¼‰
        x = torch.ones(2, 10, 64)

        output = ln(x)

        # åº”è¯¥è¿”å›žåˆç†è¾“å‡ºï¼ˆè™½ç„¶æœ‰è­¦å‘Šï¼‰
        assert output.shape == x.shape
        assert not torch.isnan(output).all()

    def test_very_large_input_values(self):
        """æµ‹è¯•å¾ˆå¤§çš„è¾“å…¥å€¼"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        # å¾ˆå¤§çš„è¾“å…¥å€¼ï¼ˆé€šè¿‡æžç«¯ token IDï¼‰
        input_ids = torch.full((1, 10), 49)  # æœ€å¤§ token ID

        logits, _ = model(input_ids)

        # åº”è¯¥è¿”å›žåˆç†è¾“å‡º
        assert not torch.isnan(logits).any()
        assert not torch.isinf(logits).any()

    def test_negative_logits(self):
        """æµ‹è¯•è´Ÿ logits"""
        # åˆ›å»ºå…¨æ˜¯è´Ÿæ•°çš„ logits
        logits = torch.tensor([[-1.0, -2.0, -3.0]])

        # åº”ç”¨ softmax
        probs = torch.softmax(logits, dim=-1)

        # æ¦‚çŽ‡åº”è¯¥å’Œä¸º 1
        assert torch.allclose(probs.sum(), torch.ones(1), atol=1e-5)

    def test_mixed_positive_negative_logits(self):
        """æµ‹è¯•æ··åˆæ­£è´Ÿ logits"""
        logits = torch.tensor([[1.0, -1.0, 2.0, -2.0]])

        probs = torch.softmax(logits, dim=-1)

        # æ¦‚çŽ‡åº”è¯¥å’Œä¸º 1
        assert torch.allclose(probs.sum(), torch.ones(1), atol=1e-5)


class TestSpecialCharacters:
    """ç‰¹æ®Šå­—ç¬¦æµ‹è¯•"""

    def test_unicode_characters(self):
        """æµ‹è¯• Unicode å­—ç¬¦"""
        tokenizer = BPETokenizer(vocab_size=100)
        texts = [
            "Hello ä¸–ç•Œ ðŸŒ",
            "æµ‹è¯• ä¸­æ–‡",
            "Emoji ðŸ˜Š ðŸŽ‰",
        ]
        tokenizer.fit(texts, verbose=False)

        for text in texts:
            tokens = tokenizer.encode(text)
            assert len(tokens) > 0

    def test_newlines_and_tabs(self):
        """æµ‹è¯•æ¢è¡Œç¬¦å’Œåˆ¶è¡¨ç¬¦"""
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹\tè¯•\næ¢ è¡Œ"], verbose=False)

        text = "æµ‹\tè¯•\næ¢ è¡Œ"
        tokens = tokenizer.encode(text)

        assert len(tokens) > 0

    def test_repeated_characters(self):
        """æµ‹è¯•é‡å¤å­—ç¬¦"""
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        text = "å•Šå•Šå•Šå•Šå•Šå•Š"
        tokens = tokenizer.encode(text)

        assert len(tokens) > 0


class TestModelEdgeCases:
    """æ¨¡åž‹è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def test_single_layer_model(self):
        """æµ‹è¯•å•å±‚æ¨¡åž‹"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=1,  # åªæœ‰ä¸€å±‚
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)

        input_ids = torch.randint(0, 50, (2, 10))
        logits, _ = model(input_ids)

        assert logits.shape == (2, 10, 50)

    def test_single_attention_head(self):
        """æµ‹è¯•å•æ³¨æ„åŠ›å¤´æ¨¡åž‹"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=1,  # åªæœ‰ä¸€ä¸ªå¤´
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)

        input_ids = torch.randint(0, 50, (2, 10))
        logits, _ = model(input_ids)

        assert logits.shape == (2, 10, 50)

    def test_minimum_vocabulary_size(self):
        """æµ‹è¯•æœ€å°è¯è¡¨å¤§å°"""
        config = GPTConfig(
            vocab_size=10,  # éžå¸¸å°çš„è¯è¡¨
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)

        input_ids = torch.randint(0, 10, (2, 10))
        logits, _ = model(input_ids)

        assert logits.shape == (2, 10, 10)

    def test_large_embedding_dimension(self):
        """æµ‹è¯•å¤§åµŒå…¥ç»´åº¦"""
        config = GPTConfig(
            vocab_size=50,
            emb_dim=256,  # è¾ƒå¤§çš„åµŒå…¥ç»´åº¦
            num_heads=8,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)

        input_ids = torch.randint(0, 50, (1, 10))
        logits, _ = model(input_ids)

        assert logits.shape == (1, 10, 50)


class TestGenerationEdgeCases:
    """ç”Ÿæˆè¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.fixture
    def generator(self):
        config = GPTConfig(
            vocab_size=50,
            emb_dim=32,
            num_heads=2,
            num_layers=2,
            context_size=32,
            dropout=0.0
        )
        model = GPT(config)
        model.eval()

        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯• æ–‡ æœ¬"], verbose=False)

        return TextGenerator(model, tokenizer, torch.device("cpu"))

    def test_max_length_equals_prompt_length(self, generator):
        """æµ‹è¯•æœ€å¤§é•¿åº¦ç­‰äºŽæç¤ºé•¿åº¦"""
        prompt = "æµ‹"

        # èŽ·å–æç¤ºé•¿åº¦
        prompt_len = len(generator.tokenizer.encode(prompt))

        # è®¾ç½® max_length ç­‰äºŽæç¤ºé•¿åº¦
        result = generator.generate(prompt, max_length=prompt_len)

        # åº”è¯¥è¿”å›žåŽŸå§‹æç¤ºï¼ˆä¸ç”Ÿæˆæ–°å†…å®¹ï¼‰
        assert len(result) > 0

    def test_max_length_less_than_prompt(self, generator):
        """æµ‹è¯•æœ€å¤§é•¿åº¦å°äºŽæç¤ºé•¿åº¦"""
        prompt = "æµ‹ è¯• æ–‡ æœ¬"

        # è®¾ç½®å¾ˆå°çš„ max_length
        result = generator.generate(prompt, max_length=2)

        # åº”è¯¥è¿”å›žæˆªæ–­çš„å†…å®¹
        assert len(result) >= 0

    def test_unknown_tokens(self, generator):
        """æµ‹è¯•æœªçŸ¥ token"""
        # ä½¿ç”¨ä¸åœ¨è®­ç»ƒé›†ä¸­çš„å­—ç¬¦
        result = generator.generate("xyz", max_length=10)

        # åº”è¯¥ä»ç„¶ç”Ÿæˆå†…å®¹ï¼ˆä½¿ç”¨ UNKï¼‰
        assert isinstance(result, str)

    def test_generation_with_eos_immediate(self, generator):
        """æµ‹è¯•ç«‹å³é‡åˆ° EOS"""
        # è®¾ç½® EOS ä¸ºç¬¬ä¸€ä¸ª token
        result = generator.generate("æµ‹", max_length=10, eos_token_id=0)

        # åº”è¯¥è¿”å›žç»“æžœ
        assert isinstance(result, str)


class TestBatchEdgeCases:
    """æ‰¹æ¬¡è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def test_single_sample_batch(self):
        """æµ‹è¯•å•æ ·æœ¬æ‰¹æ¬¡"""
        from train import collate_fn

        batch = [
            (torch.tensor([1, 2, 3]), torch.tensor([2, 3, 4])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        assert padded_inputs.shape == (1, 3)
        assert padded_targets.shape == (1, 3)

    def test_very_large_batch(self):
        """æµ‹è¯•éžå¸¸å¤§çš„æ‰¹æ¬¡"""
        from train import collate_fn

        # åˆ›å»º 100 ä¸ªæ ·æœ¬
        batch = [
            (torch.tensor([1, 2]), torch.tensor([2, 3]))
            for _ in range(100)
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        assert padded_inputs.shape[0] == 100

    def test_variable_length_batch(self):
        """æµ‹è¯•å˜é•¿æ‰¹æ¬¡"""
        from train import collate_fn

        batch = [
            (torch.tensor([1]), torch.tensor([2])),
            (torch.tensor([1, 2, 3, 4, 5]), torch.tensor([2, 3, 4, 5, 6])),
            (torch.tensor([1, 2]), torch.tensor([2, 3])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        # æ‰€æœ‰æ ·æœ¬åº”è¯¥å¡«å……åˆ°ç›¸åŒé•¿åº¦
        assert padded_inputs.shape[0] == 3
        assert padded_inputs.shape[1] == 5


class TestTokenizerEdgeCases:
    """åˆ†è¯å™¨è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    def test_tokenizer_without_training(self):
        """æµ‹è¯•æœªè®­ç»ƒçš„åˆ†è¯å™¨"""
        tokenizer = BPETokenizer(vocab_size=50)

        # æœªè®­ç»ƒæ—¶ç¼–ç åº”è¯¥è¿”å›žåŽŸå§‹å­—ç¬¦
        tokens = tokenizer.encode("æµ‹")

        # åº”è¯¥è¿”å›žä¸€äº›ä¸œè¥¿ï¼ˆå­—ç¬¦çº§ï¼‰
        assert len(tokens) >= 0

    def test_tokenizer_with_single_character(self):
        """æµ‹è¯•å•å­—ç¬¦æ–‡æœ¬"""
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•"], verbose=False)

        tokens = tokenizer.encode("æµ‹")

        assert len(tokens) > 0

    def test_tokenizer_with_repeated_pattern(self):
        """æµ‹è¯•é‡å¤æ¨¡å¼"""
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ æµ‹ æµ‹"], verbose=False)

        tokens = tokenizer.encode("æµ‹ æµ‹ æµ‹ æµ‹")

        assert len(tokens) > 0
