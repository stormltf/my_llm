"""
æ•°æ®é›†å•å…ƒæµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
1. PretrainDataset - é¢„è®­ç»ƒæ•°æ®é›†
2. SFTDataset - ç›‘ç£å¾®è°ƒæ•°æ®é›†
3. collate_fn - æ‰¹æ¬¡æ•´ç†å‡½æ•°
4. æ•°æ®åŠ è½½åŠŸèƒ½
"""

import pytest
import torch
from torch.utils.data import DataLoader

from train import (
    PretrainDataset,
    SFTDataset,
    collate_fn,
    load_pretrain_data,
    load_sft_data,
    load_reward_data,
    load_rlvf_data
)
from tokenizer import BPETokenizer


class TestPretrainDataset:
    """é¢„è®­ç»ƒæ•°æ®é›†æµ‹è¯•"""

    @pytest.fixture
    def sample_texts(self):
        """ç¤ºä¾‹è®­ç»ƒæ–‡æœ¬"""
        return [
            "æˆ‘ å–œæ¬¢ å­¦ä¹  äººå·¥æ™ºèƒ½",
            "äººå·¥æ™ºèƒ½ æ˜¯ æœªæ¥ çš„ è¶‹åŠ¿",
            "æ·±åº¦ å­¦ä¹  æ˜¯ æœºå™¨ å­¦ä¹  çš„ åˆ†æ”¯",
        ]

    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºåˆ†è¯å™¨"""
        tokenizer = BPETokenizer(vocab_size=100)
        texts = ["æˆ‘ å–œæ¬¢ å­¦ä¹ ", "äººå·¥æ™ºèƒ½ å¾ˆ æœ‰è¶£", "æ·±åº¦ å­¦ä¹  å¼ºå¤§"]
        tokenizer.fit(texts, verbose=False)
        return tokenizer

    @pytest.fixture
    def dataset(self, sample_texts, tokenizer):
        """åˆ›å»ºé¢„è®­ç»ƒæ•°æ®é›†"""
        return PretrainDataset(sample_texts, tokenizer, seq_len=8)

    def test_dataset_creation(self, dataset):
        """æµ‹è¯•æ•°æ®é›†åˆ›å»º"""
        assert dataset is not None
        assert len(dataset) > 0

    def test_dataset_length(self, dataset):
        """æµ‹è¯•æ•°æ®é›†é•¿åº¦"""
        # åºåˆ—é•¿åº¦ä¸º 8ï¼Œæ ·æœ¬æ•°åº”è¯¥å¤§äº 0
        assert len(dataset) > 0

    def test_getitem_shape(self, dataset):
        """æµ‹è¯•è·å–æ ·æœ¬çš„å½¢çŠ¶"""
        input_ids, target_ids = dataset[0]

        # æ£€æŸ¥å½¢çŠ¶
        assert input_ids.shape == (8,)
        assert target_ids.shape == (8,)

        # æ£€æŸ¥ç±»å‹
        assert input_ids.dtype == torch.long
        assert target_ids.dtype == torch.long

    def test_autoregressive_property(self, dataset):
        """æµ‹è¯•è‡ªå›å½’å±æ€§"""
        input_ids, target_ids = dataset[0]

        # ç›®æ ‡åº”è¯¥æ˜¯è¾“å…¥çš„ä¸‹ä¸€ä¸ªè¯ï¼ˆåç§»1ä½ï¼‰
        # å³ target[i] åº”è¯¥ç­‰äº input[i+1]
        assert torch.equal(target_ids[:-1], input_ids[1:])

    def test_dataloader(self, dataset):
        """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        dataloader = DataLoader(dataset, batch_size=2, shuffle=False)

        for input_ids, target_ids in dataloader:
            # æ£€æŸ¥æ‰¹æ¬¡å½¢çŠ¶
            assert input_ids.shape[0] <= 2
            assert input_ids.shape[1] == 8
            assert target_ids.shape == input_ids.shape
            break  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡

    def test_empty_texts(self, tokenizer):
        """æµ‹è¯•ç©ºæ–‡æœ¬åˆ—è¡¨"""
        dataset = PretrainDataset([], tokenizer, seq_len=8)
        assert len(dataset) == 0

    def test_single_token_texts(self, tokenizer):
        """æµ‹è¯•å• token æ–‡æœ¬"""
        dataset = PretrainDataset(["æˆ‘"], tokenizer, seq_len=8)
        # åºåˆ—é•¿åº¦ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ ·æœ¬
        assert len(dataset) == 0


class TestSFTDataset:
    """SFT æ•°æ®é›†æµ‹è¯•"""

    @pytest.fixture
    def sample_data(self):
        """ç¤ºä¾‹ SFT æ•°æ®"""
        return [
            {"user": "ä½ å¥½", "assistant": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
            {"user": "1+1ç­‰äºå¤šå°‘", "assistant": "1+1ç­‰äº2"},
            {"user": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½", "assistant": "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯"},
        ]

    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºåˆ†è¯å™¨"""
        tokenizer = BPETokenizer(vocab_size=100)
        texts = ["ä½  å¥½", "1 + 1 = 2", "äººå·¥ æ™ºèƒ½ æ˜¯ ç§‘å­¦"]
        tokenizer.fit(texts, verbose=False)
        return tokenizer

    @pytest.fixture
    def dataset(self, sample_data, tokenizer):
        """åˆ›å»º SFT æ•°æ®é›†"""
        return SFTDataset(sample_data, tokenizer, max_length=64)

    def test_dataset_creation(self, dataset):
        """æµ‹è¯•æ•°æ®é›†åˆ›å»º"""
        assert dataset is not None
        assert len(dataset) > 0

    def test_dataset_length(self, dataset):
        """æµ‹è¯•æ•°æ®é›†é•¿åº¦"""
        # åº”è¯¥æœ‰ä¸è¾“å…¥æ•°æ®ç›¸åŒæ•°é‡çš„æ ·æœ¬
        assert len(dataset) > 0

    def test_getitem_shape(self, dataset):
        """æµ‹è¯•è·å–æ ·æœ¬çš„å½¢çŠ¶"""
        input_ids, target_ids = dataset[0]

        # è¾“å…¥å’Œç›®æ ‡åº”è¯¥é•¿åº¦ç›¸åŒ
        assert input_ids.shape == target_ids.shape
        assert input_ids.dim() == 1

    def test_loss_mask_present(self, dataset):
        """æµ‹è¯• loss mask å­˜åœ¨"""
        _, target_ids = dataset[0]

        # PyTorch é»˜è®¤ä½¿ç”¨ -100 ä½œä¸º ignore_index
        target_list = target_ids.tolist()
        assert -100 in target_list, f"Expected -100 in target_ids, got {target_list}"

    def test_dataloader_with_collate(self, dataset):
        """æµ‹è¯•å¸¦ collate_fn çš„æ•°æ®åŠ è½½å™¨"""
        dataloader = DataLoader(
            dataset,
            batch_size=2,
            shuffle=False,
            collate_fn=collate_fn
        )

        for input_ids, target_ids in dataloader:
            # æ£€æŸ¥æ‰¹æ¬¡å½¢çŠ¶
            assert input_ids.shape[0] <= 2
            assert input_ids.shape == target_ids.shape
            break

    def test_empty_data(self, tokenizer):
        """æµ‹è¯•ç©ºæ•°æ®"""
        dataset = SFTDataset([], tokenizer, max_length=64)
        assert len(dataset) == 0


class TestCollateFn:
    """æ‰¹æ¬¡æ•´ç†å‡½æ•°æµ‹è¯•"""

    def test_collate_fn_basic(self):
        """æµ‹è¯•åŸºç¡€ collate_fn"""
        batch = [
            (torch.tensor([1, 2, 3]), torch.tensor([2, 3, 4])),
            (torch.tensor([5, 6]), torch.tensor([6, 7])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        # æ£€æŸ¥å½¢çŠ¶
        assert padded_inputs.shape == (2, 3)
        assert padded_targets.shape == (2, 3)

    def test_collate_fn_padding(self):
        """æµ‹è¯•å¡«å……"""
        batch = [
            (torch.tensor([1, 2]), torch.tensor([2, 3])),
            (torch.tensor([4, 5, 6, 7]), torch.tensor([5, 6, 7, 8])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        # ç¬¬ä¸€ä¸ªæ ·æœ¬åº”è¯¥è¢«å¡«å……
        assert padded_inputs[0, 2].item() == 0  # input å¡«å…… 0
        # PyTorch é»˜è®¤ä½¿ç”¨ -100 ä½œä¸º ignore_index
        assert padded_targets[0, 2].item() == -100  # target å¡«å…… ignore_index

    def test_collate_fn_single_batch(self):
        """æµ‹è¯•å•æ ·æœ¬æ‰¹æ¬¡"""
        batch = [
            (torch.tensor([1, 2, 3]), torch.tensor([2, 3, 4])),
        ]

        padded_inputs, padded_targets = collate_fn(batch)

        assert padded_inputs.shape == (1, 3)
        assert padded_targets.shape == (1, 3)

    def test_collate_fn_empty_batch(self):
        """æµ‹è¯•ç©ºæ‰¹æ¬¡"""
        with pytest.raises((ValueError, RuntimeError)):
            collate_fn([])


class TestDataLoaders:
    """æ•°æ®åŠ è½½åŠŸèƒ½æµ‹è¯•"""

    def test_load_pretrain_data(self):
        """æµ‹è¯•åŠ è½½é¢„è®­ç»ƒæ•°æ®"""
        data = load_pretrain_data()

        assert isinstance(data, list)
        if data:
            assert isinstance(data[0], str)

    def test_load_sft_data(self):
        """æµ‹è¯•åŠ è½½ SFT æ•°æ®"""
        data = load_sft_data()

        assert isinstance(data, list)
        if data:
            assert 'user' in data[0]
            assert 'assistant' in data[0]

    def test_load_reward_data(self):
        """æµ‹è¯•åŠ è½½å¥–åŠ±æ•°æ®"""
        data = load_reward_data()

        assert isinstance(data, list)

    def test_load_rlvf_data(self):
        """æµ‹è¯•åŠ è½½ RLVF æ•°æ®"""
        data = load_rlvf_data()

        assert isinstance(data, list)


class TestDatasetIntegration:
    """æ•°æ®é›†é›†æˆæµ‹è¯•"""

    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºåˆ†è¯å™¨"""
        tokenizer = BPETokenizer(vocab_size=100)
        texts = ["æµ‹è¯• æ–‡æœ¬ æ•°æ®", "é›†æˆ æµ‹è¯• åœºæ™¯"]
        tokenizer.fit(texts, verbose=False)
        return tokenizer

    def test_full_training_loop(self, tokenizer):
        """æµ‹è¯•å®Œæ•´è®­ç»ƒå¾ªç¯"""
        # åˆ›å»ºæ•°æ®é›†
        texts = ["æµ‹è¯• æ•°æ® ä¸€", "æµ‹è¯• æ•°æ® äºŒ", "æµ‹è¯• æ•°æ® ä¸‰"] * 10
        dataset = PretrainDataset(texts, tokenizer, seq_len=16)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

        batch_count = 0
        for input_ids, target_ids in dataloader:
            assert input_ids.shape[1] == 16
            assert target_ids.shape[1] == 16
            batch_count += 1
            if batch_count >= 3:
                break

        assert batch_count > 0

    def test_sft_training_loop(self, tokenizer):
        """æµ‹è¯• SFT è®­ç»ƒå¾ªç¯"""
        data = [
            {"user": "ä½ å¥½", "assistant": "ä½ å¥½ï¼"},
            {"user": "å†è§", "assistant": "å†è§ï¼"},
        ] * 5

        dataset = SFTDataset(data, tokenizer, max_length=32)
        dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)

        batch_count = 0
        for input_ids, target_ids in dataloader:
            assert input_ids.shape[0] <= 2
            batch_count += 1
            if batch_count >= 3:
                break

        assert batch_count > 0


class TestDatasetEdgeCases:
    """æ•°æ®é›†è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.fixture
    def tokenizer(self):
        """åˆ›å»ºåˆ†è¯å™¨"""
        tokenizer = BPETokenizer(vocab_size=50)
        tokenizer.fit(["æµ‹ è¯•", "è¾¹ ç•Œ"], verbose=False)
        return tokenizer

    def test_very_long_sequence(self, tokenizer):
        """æµ‹è¯•è¶…é•¿åºåˆ—"""
        data = [{"user": "é—®" * 100, "assistant": "ç­”" * 100}]
        dataset = SFTDataset(data, tokenizer, max_length=32)

        # åº”è¯¥è¢«æˆªæ–­
        input_ids, target_ids = dataset[0]
        assert len(input_ids) <= 32

    def test_unicode_characters(self, tokenizer):
        """æµ‹è¯• Unicode å­—ç¬¦"""
        data = [
            {"user": "Hello ä¸–ç•Œ", "assistant": "ä½ å¥½"},
            {"user": "ğŸš€ rocket", "assistant": "ç«ç®­"},
        ]

        dataset = SFTDataset(data, tokenizer, max_length=32)
        assert len(dataset) > 0

    def test_special_tokens(self, tokenizer):
        """æµ‹è¯•ç‰¹æ®Š token"""
        data = [
            {"user": "<|im_start|>test", "assistant": "<|im_end|>reply"},
        ]

        dataset = SFTDataset(data, tokenizer, max_length=32)
        assert len(dataset) > 0
