"""
å¤§è¯­è¨€æ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹

å®ç°å®Œæ•´çš„ 5 é˜¶æ®µè®­ç»ƒï¼š
1. Pretrain (é¢„è®­ç»ƒ) - å­¦ä¹ è¯­è¨€è§„å¾‹
2. SFT (ç›‘ç£å¾®è°ƒ) - å­¦ä¹ å¯¹è¯æ ¼å¼
3. Reward Model (å¥–åŠ±æ¨¡å‹) - å­¦ä¹ äººç±»åå¥½
4. RLHF (PPO) - ç­–ç•¥ä¼˜åŒ–
5. RLVF - å¯éªŒè¯åé¦ˆå¼ºåŒ–å­¦ä¹ 

ä½¿ç”¨æ–¹æ³•ï¼š
    # å®Œæ•´è®­ç»ƒ
    python train.py

    # è·³è¿‡ç‰¹å®šé˜¶æ®µ
    python train.py --skip-pretrain --skip-sft

    # åªè®­ç»ƒ RLHF/RLVFï¼ˆéœ€è¦å·²æœ‰ SFT æ¨¡å‹ï¼‰
    python train.py --skip-pretrain --skip-sft --skip-reward
"""

import os
import argparse
import json
from datetime import datetime
from typing import List, Dict, Optional
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from model import GPT, GPTConfig, MyLLM
from config import MyLLMConfig, get_mini_config
from tokenizer import BPETokenizer


# ==========================================
# æ•°æ®é›†ç±»
# ==========================================

class PretrainDataset(Dataset):
    """é¢„è®­ç»ƒæ•°æ®é›†"""

    def __init__(self, texts: List[str], tokenizer: BPETokenizer, seq_len: int):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = []

        print("æ­£åœ¨å¤„ç†é¢„è®­ç»ƒæ•°æ®...")
        all_token_ids = []
        for text in tqdm(texts, desc="ç¼–ç æ–‡æœ¬"):
            token_ids = tokenizer.encode(text)
            all_token_ids.extend(token_ids)

        print(f"æ€»å…±ç¼–ç äº† {len(all_token_ids)} ä¸ª token")

        for i in range(0, len(all_token_ids) - seq_len - 1):
            input_ids = all_token_ids[i:i + seq_len]
            target_ids = all_token_ids[i + 1:i + seq_len + 1]
            self.samples.append({
                'input_ids': input_ids,
                'target_ids': target_ids
            })

        print(f"ç”Ÿæˆäº† {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.tensor(sample['input_ids'], dtype=torch.long),
            torch.tensor(sample['target_ids'], dtype=torch.long)
        )


class SFTDataset(Dataset):
    """SFT æ•°æ®é›† - åªå¯¹ assistant éƒ¨åˆ†è®¡ç®— loss"""

    def __init__(self, data: List[Dict], tokenizer: BPETokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        print("æ­£åœ¨å¤„ç† SFT æ•°æ®...")
        for item in tqdm(data, desc="å¤„ç†å¯¹è¯"):
            # åˆ†åˆ«ç¼–ç ç”¨æˆ·å’ŒåŠ©æ‰‹éƒ¨åˆ†ï¼Œä»¥ä¾¿åˆ›å»º loss mask
            user_part = f"<|im_start|>user\n{item['user']}<|im_end|>\n<|im_start|>assistant\n"
            assistant_part = f"{item['assistant']}<|im_end|>"

            user_ids = tokenizer.encode(user_part)
            assistant_ids = tokenizer.encode(assistant_part)

            # å®Œæ•´åºåˆ—
            token_ids = user_ids + assistant_ids

            # æˆªæ–­
            if len(token_ids) > max_length:
                token_ids = token_ids[:max_length]
                # é‡æ–°è®¡ç®— user éƒ¨åˆ†é•¿åº¦ï¼ˆç”¨äº maskï¼‰
                user_len = min(len(user_ids), max_length - 1)
            else:
                user_len = len(user_ids)

            # æ„é€ è¾“å…¥å’Œç›®æ ‡ï¼ˆè‡ªå›å½’ï¼‰
            if len(token_ids) > 1:
                input_ids = token_ids[:-1]
                target_ids = token_ids[1:]

                # åˆ›å»º loss maskï¼šåªå¯¹ assistant éƒ¨åˆ†è®¡ç®— loss
                # user éƒ¨åˆ†çš„ target è®¾ä¸º -1ï¼ˆä¼šè¢« loss å‡½æ•°å¿½ç•¥ï¼‰
                loss_mask = [-1] * (user_len - 1) + target_ids[user_len - 1:]

                self.samples.append({
                    'input_ids': input_ids,
                    'target_ids': loss_mask  # ä½¿ç”¨å¸¦ mask çš„ target
                })

        print(f"SFT æ•°æ®é›†å¤§å°: {len(self.samples)}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.tensor(sample['input_ids'], dtype=torch.long),
            torch.tensor(sample['target_ids'], dtype=torch.long)
        )


def collate_fn(batch):
    """è‡ªå®šä¹‰ collate å‡½æ•°ï¼Œå¤„ç†å˜é•¿åºåˆ—"""
    input_ids = [item[0] for item in batch]
    target_ids = [item[1] for item in batch]

    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_len = max(len(ids) for ids in input_ids)

    # Padding
    padded_inputs = []
    padded_targets = []

    for inp, tgt in zip(input_ids, target_ids):
        pad_len = max_len - len(inp)
        padded_inputs.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_targets.append(torch.cat([tgt, torch.full((pad_len,), -1, dtype=torch.long)]))

    return torch.stack(padded_inputs), torch.stack(padded_targets)


# ==========================================
# è®­ç»ƒå‡½æ•°
# ==========================================

def load_pretrain_data() -> List[str]:
    """åŠ è½½é¢„è®­ç»ƒæ•°æ®"""
    data_path = "data/pretrain_data.txt"
    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    else:
        # ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
        corpus = [
            "æˆ‘ æ˜¯ ä¸€ä¸ª äººå·¥æ™ºèƒ½ åŠ©æ‰‹",
            "äººå·¥æ™ºèƒ½ æ˜¯ è®¡ç®—æœº ç§‘å­¦ çš„ ä¸€ä¸ª åˆ†æ”¯",
            "æ·±åº¦ å­¦ä¹  æ˜¯ æœºå™¨ å­¦ä¹  çš„ ä¸€ç§ æ–¹æ³•",
            "è‡ªç„¶ è¯­è¨€ å¤„ç† è®© è®¡ç®—æœº ç†è§£ äººç±» è¯­è¨€",
            "å¤§ è¯­è¨€ æ¨¡å‹ å¯ä»¥ ç”Ÿæˆ æµç•… çš„ æ–‡æœ¬",
        ] * 100
        return corpus


def load_sft_data() -> List[Dict]:
    """åŠ è½½ SFT æ•°æ®"""
    data_path = "data/sft_data.json"
    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("æœªæ‰¾åˆ° SFT æ•°æ®æ–‡ä»¶")
        return []


def load_reward_data() -> List[Dict]:
    """åŠ è½½å¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®"""
    data_path = "data/reward_data.json"
    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("æœªæ‰¾åˆ°å¥–åŠ±æ•°æ®æ–‡ä»¶")
        return []


def load_rlvf_data() -> List[Dict]:
    """åŠ è½½ RLVF æ•°æ®"""
    data_path = "data/rlvf_data.json"
    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("æœªæ‰¾åˆ° RLVF æ•°æ®æ–‡ä»¶")
        return []


def train_pretrain(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ

    ç›®æ ‡ï¼šå­¦ä¹ è¯­è¨€è§„å¾‹ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ (Pretrain)")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    corpus = load_pretrain_data()
    if not corpus:
        print("æ²¡æœ‰é¢„è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡")
        return {}

    dataset = PretrainDataset(corpus, tokenizer, seq_len=config.seq_len)
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0
    )

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.pretrain_lr,
        weight_decay=0.01
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.pretrain_epochs
    )

    history = {'loss': []}
    model.train()

    for epoch in range(config.pretrain_epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Pretrain Epoch {epoch + 1}/{config.pretrain_epochs}")

        for input_ids, target_ids in progress_bar:
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)

            _, loss = model(input_ids, target_ids)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        scheduler.step()
        avg_loss = total_loss / len(dataloader)
        history['loss'].append(avg_loss)
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "pretrain_final.pt")
    torch.save(model.state_dict(), save_path)
    print(f"é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {save_path}")

    return history


def train_sft(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 2ï¼šç›‘ç£å¾®è°ƒ (SFT)

    ç›®æ ‡ï¼šå­¦ä¹ å¯¹è¯æ ¼å¼ï¼Œè·å¾—æŒ‡ä»¤éµå¾ªèƒ½åŠ›
    åŒ…å«æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 2ï¼šç›‘ç£å¾®è°ƒ (SFT)")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    sft_data = load_sft_data()
    if not sft_data:
        print("æ²¡æœ‰ SFT æ•°æ®ï¼Œè·³è¿‡")
        return {}

    dataset = SFTDataset(sft_data, tokenizer, max_length=config.context_size)
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )

    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.sft_lr,
        weight_decay=0.01
    )

    history = {'loss': []}
    model.train()

    # æ—©åœå‚æ•°
    best_loss = float('inf')
    patience = 5  # è¿ç»­ 5 ä¸ª epoch æ²¡æœ‰æ”¹å–„å°±åœæ­¢
    patience_counter = 0
    min_loss_threshold = 0.1  # loss ä½äºæ­¤å€¼æ—¶å¼€å§‹ç›‘æ§è¿‡æ‹Ÿåˆ

    for epoch in range(config.sft_epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"SFT Epoch {epoch + 1}/{config.sft_epochs}")

        for input_ids, target_ids in progress_bar:
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)

            _, loss = model(input_ids, target_ids)

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(dataloader)
        history['loss'].append(avg_loss)
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")

        # æ—©åœæ£€æŸ¥ï¼ˆå½“ loss è¶³å¤Ÿä½æ—¶å¼€å§‹ç›‘æ§ï¼‰
        if avg_loss < min_loss_threshold:
            if avg_loss < best_loss - 0.01:  # éœ€è¦æ˜æ˜¾æ”¹å–„
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_path = os.path.join(config.checkpoint_dir, "sft_best.pt")
                torch.save(model.state_dict(), best_path)
            else:
                patience_counter += 1
                print(f"  âš ï¸ Loss æ”¹å–„ä¸æ˜æ˜¾ ({patience_counter}/{patience})")

            if patience_counter >= patience:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {patience} ä¸ª epoch æ²¡æœ‰æ˜æ˜¾æ”¹å–„")
                print(f"   æœ€ä½³ Loss: {best_loss:.4f}")
                # åŠ è½½æœ€ä½³æ¨¡å‹
                best_path = os.path.join(config.checkpoint_dir, "sft_best.pt")
                if os.path.exists(best_path):
                    model.load_state_dict(torch.load(best_path, map_location=device, weights_only=True))
                break

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "sft_final.pt")
    torch.save(model.state_dict(), save_path)
    print(f"SFT æ¨¡å‹å·²ä¿å­˜: {save_path}")

    return history


def train_reward_model(
    base_model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
):
    """
    é˜¶æ®µ 3ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹

    ç›®æ ‡ï¼šå­¦ä¹ äººç±»åå¥½ï¼Œèƒ½å¤Ÿç»™å›ç­”æ‰“åˆ†
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 3ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    reward_data = load_reward_data()
    if not reward_data:
        print("æ²¡æœ‰å¥–åŠ±æ•°æ®ï¼Œè·³è¿‡")
        return None

    # å¯¼å…¥å¥–åŠ±æ¨¡å‹ç›¸å…³ç±»
    from reward_model import RewardModel, RewardModelTrainer

    # åˆ›å»ºæ¨¡å‹é…ç½®ï¼ˆä¸åŸºç¡€æ¨¡å‹ä¸€è‡´ï¼‰
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),
        emb_dim=config.emb_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        context_size=config.context_size,
        dropout=config.dropout
    )

    # ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
    reward_model = RewardModel.from_pretrained(base_model, model_config)

    # è®­ç»ƒ
    trainer = RewardModelTrainer(
        reward_model,
        tokenizer,
        model_config,
        learning_rate=config.reward_lr,
        num_epochs=config.reward_epochs
    )

    trainer.train(reward_data, batch_size=config.reward_batch_size)

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "reward_model.pt")
    trainer.save_model(save_path)

    return reward_model


def train_rlhf(
    model: GPT,
    reward_model,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 4ï¼šRLHF (PPO) è®­ç»ƒ

    ç›®æ ‡ï¼šåˆ©ç”¨å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 4ï¼šRLHF (PPO) è®­ç»ƒ")
    print("=" * 60)

    if reward_model is None:
        print("æ²¡æœ‰å¥–åŠ±æ¨¡å‹ï¼Œè·³è¿‡ RLHF")
        return {}

    # å¯¼å…¥ PPO è®­ç»ƒå™¨
    from rlhf import PPOTrainer, RLHFConfig

    # ä» SFT æ•°æ®è·å–æç¤º
    sft_data = load_sft_data()
    if not sft_data:
        print("æ²¡æœ‰ SFT æ•°æ®æä¾›æç¤ºï¼Œè·³è¿‡ RLHF")
        return {}

    prompts = [item['user'] for item in sft_data]

    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),
        emb_dim=config.emb_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        context_size=config.context_size,
        dropout=0.0  # æ¨ç†æ—¶ä¸ä½¿ç”¨ dropout
    )

    # RLHF é…ç½®
    rlhf_config = RLHFConfig(
        clip_ratio=0.2,
        kl_coef=0.01,
        learning_rate=config.rlhf_lr,
        num_episodes=config.rlhf_episodes,
        batch_size=config.rlhf_batch_size,
        max_new_tokens=64
    )

    # åˆ›å»º PPO è®­ç»ƒå™¨
    trainer = PPOTrainer(
        policy_model=model,
        reward_model=reward_model,
        tokenizer=tokenizer,
        config=model_config,
        rlhf_config=rlhf_config
    )

    # è®­ç»ƒ
    history = trainer.train(prompts)

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "rlhf_final.pt")
    trainer.save_model(save_path)

    return history


def train_rlvf(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 5ï¼šRLVF è®­ç»ƒ

    ç›®æ ‡ï¼šåˆ©ç”¨å¯éªŒè¯åé¦ˆæå‡ç²¾ç¡®æ¨ç†èƒ½åŠ›
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 5ï¼šRLVF è®­ç»ƒ")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    rlvf_data = load_rlvf_data()
    if not rlvf_data:
        print("æ²¡æœ‰ RLVF æ•°æ®ï¼Œè·³è¿‡")
        return {}

    # å¯¼å…¥ RLVF è®­ç»ƒå™¨
    from rlvf import RLVFTrainer, RLVFConfig

    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),
        emb_dim=config.emb_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        context_size=config.context_size,
        dropout=0.0
    )

    # RLVF é…ç½®
    rlvf_config = RLVFConfig(
        num_iterations=config.rlvf_iterations,
        samples_per_task=2,
        correct_reward=1.0,
        incorrect_reward=-0.5,
        learning_rate=config.rlvf_lr,
        max_new_tokens=32
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RLVFTrainer(
        policy_model=model,
        tokenizer=tokenizer,
        config=model_config,
        rlvf_config=rlvf_config
    )

    # è®­ç»ƒ
    history = trainer.train(rlvf_data, batch_size=config.rlvf_batch_size)

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "rlvf_final.pt")
    trainer.save_model(save_path)

    return history


# ==========================================
# ä¸»å‡½æ•°
# ==========================================

def main():
    parser = argparse.ArgumentParser(description="MyLLM å®Œæ•´ 5 é˜¶æ®µè®­ç»ƒ")

    # é˜¶æ®µæ§åˆ¶
    parser.add_argument("--skip-pretrain", action="store_true", help="è·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")
    parser.add_argument("--skip-sft", action="store_true", help="è·³è¿‡ SFT é˜¶æ®µ")
    parser.add_argument("--skip-reward", action="store_true", help="è·³è¿‡å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
    parser.add_argument("--skip-rlhf", action="store_true", help="è·³è¿‡ RLHF é˜¶æ®µ")
    parser.add_argument("--skip-rlvf", action="store_true", help="è·³è¿‡ RLVF é˜¶æ®µ")

    # æ¨¡å‹å‚æ•°
    parser.add_argument("--vocab_size", type=int, default=6400, help="è¯è¡¨å¤§å°")
    parser.add_argument("--emb_dim", type=int, default=256, help="åµŒå…¥ç»´åº¦")
    parser.add_argument("--num_heads", type=int, default=4, help="æ³¨æ„åŠ›å¤´æ•°")
    parser.add_argument("--num_layers", type=int, default=4, help="Transformer å±‚æ•°")
    parser.add_argument("--context_size", type=int, default=256, help="ä¸Šä¸‹æ–‡é•¿åº¦")
    parser.add_argument("--dropout", type=float, default=0.1, help="Dropout æ¯”ä¾‹")

    # é€šç”¨è®­ç»ƒå‚æ•°
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--seq_len", type=int, default=64, help="åºåˆ—é•¿åº¦")

    # é¢„è®­ç»ƒå‚æ•°
    parser.add_argument("--pretrain_epochs", type=int, default=10, help="é¢„è®­ç»ƒè½®æ•°")
    parser.add_argument("--pretrain_lr", type=float, default=3e-4, help="é¢„è®­ç»ƒå­¦ä¹ ç‡")

    # SFT å‚æ•° (æ³¨æ„ï¼šepoch è¿‡å¤šä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼)
    parser.add_argument("--sft_epochs", type=int, default=20, help="SFT è®­ç»ƒè½®æ•°ï¼ˆå»ºè®® 15-30ï¼‰")
    parser.add_argument("--sft_lr", type=float, default=5e-5, help="SFT å­¦ä¹ ç‡")

    # å¥–åŠ±æ¨¡å‹å‚æ•° (å¢åŠ è½®æ¬¡ä»¥æ›´å¥½åœ°å­¦ä¹ åå¥½)
    parser.add_argument("--reward_epochs", type=int, default=15, help="å¥–åŠ±æ¨¡å‹è®­ç»ƒè½®æ•°")
    parser.add_argument("--reward_lr", type=float, default=1e-5, help="å¥–åŠ±æ¨¡å‹å­¦ä¹ ç‡")
    parser.add_argument("--reward_batch_size", type=int, default=4, help="å¥–åŠ±æ¨¡å‹æ‰¹æ¬¡å¤§å°")

    # RLHF å‚æ•° (å¢åŠ è½®æ¬¡ä»¥è·å¾—æ›´å¥½çš„å¯¹é½æ•ˆæœ)
    parser.add_argument("--rlhf_episodes", type=int, default=100, help="RLHF è®­ç»ƒè½®æ•°")
    parser.add_argument("--rlhf_lr", type=float, default=1e-5, help="RLHF å­¦ä¹ ç‡")
    parser.add_argument("--rlhf_batch_size", type=int, default=4, help="RLHF æ‰¹æ¬¡å¤§å°")

    # RLVF å‚æ•° (å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æå‡æ¨ç†èƒ½åŠ›)
    parser.add_argument("--rlvf_iterations", type=int, default=60, help="RLVF è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--rlvf_lr", type=float, default=1e-5, help="RLVF å­¦ä¹ ç‡")
    parser.add_argument("--rlvf_batch_size", type=int, default=4, help="RLVF æ‰¹æ¬¡å¤§å°")

    # è·¯å¾„
    parser.add_argument("--checkpoint_dir", type=str, default="checkpoints", help="æ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--vocab_path", type=str, default="checkpoints/vocab.json", help="è¯è¡¨è·¯å¾„")

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºç›®å½•
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    # ==========================================
    # å‡†å¤‡åˆ†è¯å™¨
    # ==========================================
    print("\n" + "=" * 60)
    print("å‡†å¤‡åˆ†è¯å™¨")
    print("=" * 60)

    if os.path.exists(args.vocab_path):
        print(f"åŠ è½½å·²æœ‰åˆ†è¯å™¨: {args.vocab_path}")
        tokenizer = BPETokenizer.load(args.vocab_path)
    else:
        print("è®­ç»ƒæ–°åˆ†è¯å™¨...")
        corpus = load_pretrain_data()
        tokenizer = BPETokenizer(vocab_size=args.vocab_size)
        tokenizer.fit(corpus, verbose=True)
        tokenizer.save(args.vocab_path)

    print(f"è¯è¡¨å¤§å°: {len(tokenizer.vocab)}")

    # æ›´æ–° vocab_size ä¸ºå®é™…å¤§å°
    args.vocab_size = len(tokenizer.vocab)

    # ==========================================
    # åˆ›å»ºæ¨¡å‹
    # ==========================================
    print("\n" + "=" * 60)
    print("åˆ›å»ºæ¨¡å‹")
    print("=" * 60)

    model_config = GPTConfig(
        vocab_size=args.vocab_size,
        emb_dim=args.emb_dim,
        num_heads=args.num_heads,
        num_layers=args.num_layers,
        context_size=args.context_size,
        dropout=args.dropout
    )

    model = GPT(model_config).to(device)
    print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}")

    # å¦‚æœè·³è¿‡é¢„è®­ç»ƒï¼Œå°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
    if args.skip_pretrain:
        pretrain_path = os.path.join(args.checkpoint_dir, "pretrain_final.pt")
        if os.path.exists(pretrain_path):
            print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrain_path}")
            model.load_state_dict(torch.load(pretrain_path, map_location=device, weights_only=True))

    if args.skip_sft:
        sft_path = os.path.join(args.checkpoint_dir, "sft_final.pt")
        if os.path.exists(sft_path):
            print(f"åŠ è½½ SFT æ¨¡å‹: {sft_path}")
            model.load_state_dict(torch.load(sft_path, map_location=device, weights_only=True))

    # ==========================================
    # å¼€å§‹è®­ç»ƒ
    # ==========================================

    # é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ
    if not args.skip_pretrain:
        train_pretrain(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")

    # é˜¶æ®µ 2ï¼šSFT
    if not args.skip_sft:
        train_sft(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ SFT é˜¶æ®µ")

    # é˜¶æ®µ 3ï¼šå¥–åŠ±æ¨¡å‹
    reward_model = None
    if not args.skip_reward:
        reward_model = train_reward_model(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
        # å°è¯•åŠ è½½å·²æœ‰å¥–åŠ±æ¨¡å‹
        reward_path = os.path.join(args.checkpoint_dir, "reward_model.pt")
        if os.path.exists(reward_path) and not args.skip_rlhf:
            from reward_model import RewardModel
            model_config = MyLLMConfig(
                vocab_size=args.vocab_size,
                emb_dim=args.emb_dim,
                num_heads=args.num_heads,
                num_layers=args.num_layers,
                context_size=args.context_size
            )
            reward_model = RewardModel(model_config)
            reward_model.load_state_dict(torch.load(reward_path, map_location=device, weights_only=True))
            reward_model.to(device)
            print(f"åŠ è½½å·²æœ‰å¥–åŠ±æ¨¡å‹: {reward_path}")

    # é˜¶æ®µ 4ï¼šRLHF
    if not args.skip_rlhf:
        train_rlhf(model, reward_model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ RLHF é˜¶æ®µ")

    # é˜¶æ®µ 5ï¼šRLVF
    if not args.skip_rlvf:
        train_rlvf(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ RLVF é˜¶æ®µ")

    # ==========================================
    # è®­ç»ƒå®Œæˆ
    # ==========================================
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"\næ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: {args.checkpoint_dir}/")
    print("  - pretrain_final.pt  (é¢„è®­ç»ƒæ¨¡å‹)")
    print("  - sft_final.pt       (SFT æ¨¡å‹)")
    print("  - reward_model.pt    (å¥–åŠ±æ¨¡å‹)")
    print("  - rlhf_final.pt      (RLHF æ¨¡å‹)")
    print("  - rlvf_final.pt      (RLVF æ¨¡å‹)")


if __name__ == "__main__":
    main()
