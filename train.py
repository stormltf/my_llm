"""
å¤§è¯­è¨€æ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹

å®ç°å®Œæ•´çš„ 5 é˜¶æ®µè®­ç»ƒï¼š
1. Pretrain (é¢„è®­ç»ƒ) - å­¦ä¹ è¯­è¨€è§„å¾‹
2. SFT (ç›‘ç£å¾®è°ƒ) - å­¦ä¹ å¯¹è¯æ ¼å¼
3. Reward Model (å¥–åŠ±æ¨¡å‹) - å­¦ä¹ äººç±»åå¥½
4. RLHF (PPO) - ç­–ç•¥ä¼˜åŒ–
5. RLVF - å¯éªŒè¯åé¦ˆå¼ºåŒ–å­¦ä¹ 

ä½¿ç”¨æ–¹æ³•ï¼š
    # å®Œæ•´è®­ç»ƒ
    python train.py

    # è·³è¿‡ç‰¹å®šé˜¶æ®µ
    python train.py --skip-pretrain --skip-sft

    # åªè®­ç»ƒ RLHF/RLVFï¼ˆéœ€è¦å·²æœ‰ SFT æ¨¡å‹ï¼‰
    python train.py --skip-pretrain --skip-sft --skip-reward
"""

# ============================================================
# æ ‡å‡†åº“å¯¼å…¥
# ============================================================
import os                          # æ“ä½œç³»ç»Ÿæ¥å£ï¼šæ–‡ä»¶è·¯å¾„ã€ç›®å½•æ“ä½œ
import argparse                    # å‘½ä»¤è¡Œå‚æ•°è§£æ
import json                        # JSON æ–‡ä»¶è¯»å†™
from datetime import datetime      # æ—¥æœŸæ—¶é—´å¤„ç†
from typing import List, Dict, Optional  # ç±»å‹æ³¨è§£ï¼Œæé«˜ä»£ç å¯è¯»æ€§
from tqdm import tqdm              # è¿›åº¦æ¡æ˜¾ç¤ºåº“

# ============================================================
# PyTorch ç›¸å…³å¯¼å…¥
# ============================================================
import torch                       # PyTorch æ ¸å¿ƒåº“
import torch.nn as nn              # ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå±‚ã€æŸå¤±å‡½æ•°ç­‰ï¼‰
from torch.utils.data import Dataset, DataLoader  # æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨

# ============================================================
# æœ¬é¡¹ç›®æ¨¡å—å¯¼å…¥
# ============================================================
from model import GPT, GPTConfig, MyLLM       # GPT æ¨¡å‹å®šä¹‰
from config import MyLLMConfig, get_mini_config  # é…ç½®ç±»
from tokenizer import BPETokenizer            # BPE åˆ†è¯å™¨


# ==========================================
# æ•°æ®é›†ç±»
# ==========================================

class PretrainDataset(Dataset):
    """
    é¢„è®­ç»ƒæ•°æ®é›†

    åŠŸèƒ½ï¼šå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„è®­ç»ƒæ ·æœ¬

    é¢„è®­ç»ƒä»»åŠ¡ï¼šè¯­è¨€å»ºæ¨¡ï¼ˆLanguage Modelingï¼‰
    --------------------------------------
    ç»™å®šå‰ n ä¸ªè¯ï¼Œé¢„æµ‹ç¬¬ n+1 ä¸ªè¯

    æ•°æ®å¤„ç†æµç¨‹ï¼š
    -------------
    åŸå§‹æ–‡æœ¬: "æˆ‘ å–œæ¬¢ å­¦ä¹  äººå·¥æ™ºèƒ½"
        â†“ tokenize
    Token IDs: [101, 234, 567, 890, 123]
        â†“ æ»‘åŠ¨çª—å£åˆ‡åˆ†
    æ ·æœ¬1: input=[101,234,567], target=[234,567,890]
    æ ·æœ¬2: input=[234,567,890], target=[567,890,123]
    """

    def __init__(self, texts: List[str], tokenizer: BPETokenizer, seq_len: int):
        """
        åˆå§‹åŒ–é¢„è®­ç»ƒæ•°æ®é›†

        å‚æ•°:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ®µæ–‡æœ¬
            tokenizer: BPE åˆ†è¯å™¨ï¼Œç”¨äºå°†æ–‡æœ¬è½¬ä¸º token ID
            seq_len: åºåˆ—é•¿åº¦ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„ token æ•°é‡
        """
        self.tokenizer = tokenizer  # ä¿å­˜åˆ†è¯å™¨å¼•ç”¨
        self.seq_len = seq_len      # ä¿å­˜åºåˆ—é•¿åº¦
        self.samples = []           # å­˜å‚¨å¤„ç†åçš„è®­ç»ƒæ ·æœ¬

        # ============================================================
        # Step 1: å°†æ‰€æœ‰æ–‡æœ¬ç¼–ç ä¸º token ID åºåˆ—
        # ============================================================
        print("æ­£åœ¨å¤„ç†é¢„è®­ç»ƒæ•°æ®...")
        all_token_ids = []          # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬çš„ token IDï¼ˆæ‹¼æ¥æˆä¸€ä¸ªé•¿åºåˆ—ï¼‰

        for text in tqdm(texts, desc="ç¼–ç æ–‡æœ¬"):  # tqdm æ˜¾ç¤ºè¿›åº¦æ¡
            token_ids = tokenizer.encode(text)     # å°†æ–‡æœ¬è½¬ä¸º token ID åˆ—è¡¨
            all_token_ids.extend(token_ids)        # è¿½åŠ åˆ°æ€»åºåˆ—ä¸­

        print(f"æ€»å…±ç¼–ç äº† {len(all_token_ids)} ä¸ª token")

        # ============================================================
        # Step 2: ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†è®­ç»ƒæ ·æœ¬
        # ============================================================
        # è‡ªå›å½’è®­ç»ƒï¼šç”¨ token[i:i+seq_len] é¢„æµ‹ token[i+1:i+seq_len+1]
        #
        # ä¸¾ä¾‹ï¼ˆseq_len=3ï¼‰ï¼š
        #   all_token_ids = [A, B, C, D, E, F, G]
        #
        #   i=0: input=[A,B,C], target=[B,C,D]  # ç”¨ABCé¢„æµ‹BCD
        #   i=1: input=[B,C,D], target=[C,D,E]  # ç”¨BCDé¢„æµ‹CDE
        #   i=2: input=[C,D,E], target=[D,E,F]  # ç”¨CDEé¢„æµ‹DEF
        #   ...
        for i in range(0, len(all_token_ids) - seq_len - 1):
            # è¾“å…¥åºåˆ—ï¼šä»ä½ç½® i å¼€å§‹ï¼Œå– seq_len ä¸ª token
            input_ids = all_token_ids[i:i + seq_len]
            # ç›®æ ‡åºåˆ—ï¼šä»ä½ç½® i+1 å¼€å§‹ï¼Œå– seq_len ä¸ª tokenï¼ˆå‘ååç§»1ä½ï¼‰
            target_ids = all_token_ids[i + 1:i + seq_len + 1]

            # ä¿å­˜ä¸ºå­—å…¸æ ¼å¼
            self.samples.append({
                'input_ids': input_ids,
                'target_ids': target_ids
            })

        print(f"ç”Ÿæˆäº† {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")

    def __len__(self):
        """
        è¿”å›æ•°æ®é›†å¤§å°

        PyTorch DataLoader éœ€è¦è¿™ä¸ªæ–¹æ³•æ¥çŸ¥é“æœ‰å¤šå°‘æ ·æœ¬
        """
        return len(self.samples)

    def __getitem__(self, idx):
        """
        è·å–æŒ‡å®šç´¢å¼•çš„æ ·æœ¬

        å‚æ•°:
            idx: æ ·æœ¬ç´¢å¼•ï¼Œ0 åˆ° len(dataset)-1

        è¿”å›:
            (input_tensor, target_tensor) å…ƒç»„
            - input_tensor: å½¢çŠ¶ [seq_len]ï¼Œè¾“å…¥ token ID
            - target_tensor: å½¢çŠ¶ [seq_len]ï¼Œç›®æ ‡ token ID

        PyTorch DataLoader ä¼šè°ƒç”¨è¿™ä¸ªæ–¹æ³•æ¥è·å–æ¯ä¸ªæ ·æœ¬
        """
        sample = self.samples[idx]
        return (
            # torch.tensor() å°† Python åˆ—è¡¨è½¬ä¸º PyTorch å¼ é‡
            # dtype=torch.long è¡¨ç¤º 64 ä½æ•´æ•°ï¼ˆtoken ID å¿…é¡»æ˜¯æ•´æ•°ï¼‰
            torch.tensor(sample['input_ids'], dtype=torch.long),
            torch.tensor(sample['target_ids'], dtype=torch.long)
        )


class SFTDataset(Dataset):
    """
    SFT (Supervised Fine-Tuning) æ•°æ®é›†

    æ ¸å¿ƒè®¾è®¡ï¼šåªå¯¹ assistant å›å¤éƒ¨åˆ†è®¡ç®— loss

    ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
    ---------------
    1. æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¼š"å¦‚ä½•å›ç­”"ï¼Œè€Œä¸æ˜¯"å¦‚ä½•æé—®"
    2. ç”¨æˆ·çš„è¾“å…¥æ˜¯å·²çŸ¥çš„ï¼Œä¸éœ€è¦æ¨¡å‹å»é¢„æµ‹
    3. åªåœ¨ assistant éƒ¨åˆ†è®¡ç®— loss å¯ä»¥ï¼š
       - æ›´é«˜æ•ˆåœ°åˆ©ç”¨æ¢¯åº¦æ›´æ–°
       - é¿å…æ¨¡å‹å­¦ä¹ å¤è¿°ç”¨æˆ·è¾“å…¥
       - è®©æ¨¡å‹ä¸“æ³¨äºç”Ÿæˆé«˜è´¨é‡å›å¤

    æ•°æ®å¤„ç†æµç¨‹ç¤ºæ„ï¼š
    -----------------
    åŸå§‹å¯¹è¯:
        user: "ä½ å¥½"
        assistant: "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"

    ç¼–ç åçš„ token åºåˆ—:
        [<im_start>, user, \\n, ä½ , å¥½, <im_end>, \\n, <im_start>, assistant, \\n, ä½ , å¥½, ï¼, æœ‰, ...]
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ user_part â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”€ assistant_part â”€â”€â”€â”€â”€â”€â”¤
        â”‚                                            â”‚
        â”‚        è¿™éƒ¨åˆ† loss è®¾ä¸º -1 (å¿½ç•¥)          â”‚  è¿™éƒ¨åˆ†æ­£å¸¸è®¡ç®— loss
    """

    def __init__(self, data: List[Dict], tokenizer: BPETokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []

        print("æ­£åœ¨å¤„ç† SFT æ•°æ®...")
        for item in tqdm(data, desc="å¤„ç†å¯¹è¯"):
            # ============================================================
            # Step 1: åˆ†åˆ«ç¼–ç ç”¨æˆ·å’ŒåŠ©æ‰‹éƒ¨åˆ†
            # ============================================================
            # ä½¿ç”¨ ChatML æ ¼å¼ï¼š<|im_start|>role\ncontent<|im_end|>
            # è¿™ç§æ ¼å¼è®©æ¨¡å‹èƒ½å¤ŸåŒºåˆ†ä¸åŒè§’è‰²çš„å‘è¨€
            user_part = f"<|im_start|>user\n{item['user']}<|im_end|>\n<|im_start|>assistant\n"
            assistant_part = f"{item['assistant']}<|im_end|>"

            user_ids = tokenizer.encode(user_part)
            assistant_ids = tokenizer.encode(assistant_part)

            # å®Œæ•´åºåˆ— = user_part + assistant_part
            token_ids = user_ids + assistant_ids

            # ============================================================
            # Step 2: æˆªæ–­å¤„ç†ï¼ˆé˜²æ­¢è¶…è¿‡æœ€å¤§é•¿åº¦ï¼‰
            # ============================================================
            if len(token_ids) > max_length:
                token_ids = token_ids[:max_length]
                # é‡æ–°è®¡ç®— user éƒ¨åˆ†é•¿åº¦ï¼ˆç”¨äºåç»­åˆ›å»º maskï¼‰
                user_len = min(len(user_ids), max_length - 1)
            else:
                user_len = len(user_ids)

            # ============================================================
            # Step 3: æ„é€ è‡ªå›å½’è®­ç»ƒæ ·æœ¬
            # ============================================================
            # è‡ªå›å½’ï¼šç”¨ token[0:n-1] é¢„æµ‹ token[1:n]
            #
            # ä¸¾ä¾‹ï¼ˆå‡è®¾ token_ids = [A, B, C, D, E]ï¼‰ï¼š
            #   input_ids  = [A, B, C, D]     (å‰ n-1 ä¸ª)
            #   target_ids = [B, C, D, E]     (å n-1 ä¸ªï¼Œå‘ååç§»1ä½)
            #
            # è¿™æ ·æ¨¡å‹å­¦ä¹ ï¼šç»™å®š A é¢„æµ‹ Bï¼Œç»™å®š AB é¢„æµ‹ Cï¼Œä»¥æ­¤ç±»æ¨
            if len(token_ids) > 1:
                input_ids = token_ids[:-1]   # å»æ‰æœ€åä¸€ä¸ª
                target_ids = token_ids[1:]   # å»æ‰ç¬¬ä¸€ä¸ª

                # ============================================================
                # Step 4: åˆ›å»º Loss Mask
                # ============================================================
                # å…³é”®ï¼šåªå¯¹ assistant éƒ¨åˆ†è®¡ç®— loss
                #
                # å‡è®¾ user_len = 5ï¼Œtoken åºåˆ—å¦‚ä¸‹ï¼š
                #   ä½ç½®:    0    1    2    3    4  â”‚  5    6    7    8
                #   Token: [u1] [u2] [u3] [u4] [u5] â”‚ [a1] [a2] [a3] [a4]
                #          â†â”€â”€â”€â”€â”€ user_part â”€â”€â”€â”€â”€â†’  â”‚ â†â”€â”€ assistant_part â”€â”€â†’
                #
                # è‡ªå›å½’åç§»åï¼š
                #   input:   [u1] [u2] [u3] [u4] [u5] [a1] [a2] [a3]
                #   target:  [u2] [u3] [u4] [u5] [a1] [a2] [a3] [a4]
                #            â”œâ”€â”€â”€ å¿½ç•¥(mask=-1) â”€â”€â”¤ â”œâ”€ è®¡ç®—loss â”€â”¤
                #            ä½ç½® 0 åˆ° user_len-2     ä½ç½® user_len-1 å¼€å§‹
                #
                # PyTorch çš„ CrossEntropyLoss ä¼šè‡ªåŠ¨å¿½ç•¥ target=-100 çš„ä½ç½®
                loss_mask = [-100] * (user_len - 1) + target_ids[user_len - 1:]

                self.samples.append({
                    'input_ids': input_ids,
                    'target_ids': loss_mask  # ä½¿ç”¨å¸¦ mask çš„ target
                })

        print(f"SFT æ•°æ®é›†å¤§å°: {len(self.samples)}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        return (
            torch.tensor(sample['input_ids'], dtype=torch.long),
            torch.tensor(sample['target_ids'], dtype=torch.long)
        )


def collate_fn(batch):
    """
    è‡ªå®šä¹‰æ‰¹æ¬¡æ•´ç†å‡½æ•°ï¼ˆCollate Functionï¼‰

    ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‡½æ•°ï¼Ÿ
    ------------------
    1. DataLoader é»˜è®¤ä¼šæŠŠå¤šä¸ªæ ·æœ¬å †å æˆä¸€ä¸ªæ‰¹æ¬¡
    2. ä½†å †å è¦æ±‚æ‰€æœ‰æ ·æœ¬é•¿åº¦ç›¸åŒ
    3. SFT æ•°æ®é›†ä¸­æ¯ä¸ªå¯¹è¯é•¿åº¦ä¸åŒï¼Œéœ€è¦æ‰‹åŠ¨å¤„ç†

    å¤„ç†æµç¨‹ï¼š
    ---------
    è¾“å…¥ batchï¼ˆå‡è®¾ batch_size=3ï¼‰ï¼š
        æ ·æœ¬1: [A, B, C]        (é•¿åº¦ 3)
        æ ·æœ¬2: [D, E, F, G, H]  (é•¿åº¦ 5)
        æ ·æœ¬3: [I, J]           (é•¿åº¦ 2)

    å¤„ç†åï¼ˆå¡«å……åˆ°æœ€å¤§é•¿åº¦ 5ï¼‰ï¼š
        æ ·æœ¬1: [A, B, C, 0, 0]     target: [B, C, D, -1, -1]
        æ ·æœ¬2: [D, E, F, G, H]     target: [E, F, G, H, I]
        æ ·æœ¬3: [I, J, 0, 0, 0]     target: [J, K, -1, -1, -1]

    æ³¨æ„ï¼štarget ä¸­çš„ -1 ä¼šè¢« CrossEntropyLoss å¿½ç•¥ï¼ˆignore_index=-1ï¼‰

    å‚æ•°:
        batch: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (input_ids, target_ids) å…ƒç»„

    è¿”å›:
        (padded_inputs, padded_targets) å…ƒç»„
        - padded_inputs: [batch_size, max_len]
        - padded_targets: [batch_size, max_len]
    """
    # ä» batch ä¸­åˆ†ç¦»å‡ºæ‰€æœ‰çš„ input å’Œ target
    input_ids = [item[0] for item in batch]   # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¼ é‡
    target_ids = [item[1] for item in batch]

    # æ‰¾åˆ°è¿™ä¸ªæ‰¹æ¬¡ä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_len = max(len(ids) for ids in input_ids)

    # å‡†å¤‡å¡«å……åçš„åˆ—è¡¨
    padded_inputs = []
    padded_targets = []

    # é€ä¸ªæ ·æœ¬è¿›è¡Œå¡«å……
    for inp, tgt in zip(input_ids, target_ids):
        # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦
        pad_len = max_len - len(inp)

        # å¡«å…… inputï¼šç”¨ 0ï¼ˆé€šå¸¸æ˜¯ <PAD> tokenï¼‰
        # torch.cat æ‹¼æ¥ä¸¤ä¸ªå¼ é‡
        padded_inputs.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))

        # å¡«å…… targetï¼šç”¨ -100ï¼ˆè®© loss å‡½æ•°å¿½ç•¥è¿™äº›ä½ç½®ï¼‰
        # torch.full åˆ›å»ºä¸€ä¸ªå¡«æ»¡æŒ‡å®šå€¼çš„å¼ é‡
        padded_targets.append(torch.cat([tgt, torch.full((pad_len,), -100, dtype=torch.long)]))

    # torch.stack å°†åˆ—è¡¨ä¸­çš„å¼ é‡å †å æˆä¸€ä¸ªæ‰¹æ¬¡å¼ é‡
    # [tensor1, tensor2, tensor3] â†’ [3, max_len]
    return torch.stack(padded_inputs), torch.stack(padded_targets)


# ==========================================
# æ•°æ®åŠ è½½å‡½æ•°
# ==========================================
# æ¯ä¸ªè®­ç»ƒé˜¶æ®µéœ€è¦ä¸åŒæ ¼å¼çš„æ•°æ®ï¼š
#
# é˜¶æ®µ         | æ•°æ®æ ¼å¼                    | æ–‡ä»¶
# ------------|---------------------------|------------------
# Pretrain    | çº¯æ–‡æœ¬                      | pretrain_data.txt
# SFT         | {"user": ..., "assistant": ...}  | sft_data.json
# Reward      | {"prompt": ..., "chosen": ..., "rejected": ...} | reward_data.json
# RLVF        | {"question": ..., "answer": ...} | rlvf_data.json

def load_pretrain_data() -> List[str]:
    """
    åŠ è½½é¢„è®­ç»ƒæ•°æ®

    æ•°æ®æ ¼å¼ï¼šæ¯è¡Œä¸€æ®µæ–‡æœ¬
    ç¤ºä¾‹æ–‡ä»¶å†…å®¹ï¼š
        æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹
        äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§æ–¹æ³•

    è¿”å›:
        æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€è¡Œæ–‡æœ¬
    """
    data_path = "data/pretrain_data.txt"  # æ•°æ®æ–‡ä»¶è·¯å¾„

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if os.path.exists(data_path):
        # è¯»å–æ–‡ä»¶ï¼ŒæŒ‰è¡Œåˆ†å‰²ï¼Œå»é™¤ç©ºè¡Œå’Œé¦–å°¾ç©ºç™½
        with open(data_path, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip()]
    else:
        # æ–‡ä»¶ä¸å­˜åœ¨æ—¶ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        print("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹æ•°æ®")
        corpus = [
            "æˆ‘ æ˜¯ ä¸€ä¸ª äººå·¥æ™ºèƒ½ åŠ©æ‰‹",
            "äººå·¥æ™ºèƒ½ æ˜¯ è®¡ç®—æœº ç§‘å­¦ çš„ ä¸€ä¸ª åˆ†æ”¯",
            "æ·±åº¦ å­¦ä¹  æ˜¯ æœºå™¨ å­¦ä¹  çš„ ä¸€ç§ æ–¹æ³•",
            "è‡ªç„¶ è¯­è¨€ å¤„ç† è®© è®¡ç®—æœº ç†è§£ äººç±» è¯­è¨€",
            "å¤§ è¯­è¨€ æ¨¡å‹ å¯ä»¥ ç”Ÿæˆ æµç•… çš„ æ–‡æœ¬",
        ] * 100  # é‡å¤ 100 æ¬¡å¢åŠ æ•°æ®é‡
        return corpus


def load_sft_data() -> List[Dict]:
    """
    åŠ è½½ SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰æ•°æ®

    æ•°æ®æ ¼å¼ï¼šJSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« user å’Œ assistant å­—æ®µ
    ç¤ºä¾‹æ–‡ä»¶å†…å®¹ï¼š
        [
            {"user": "ä½ å¥½", "assistant": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
            {"user": "1+1ç­‰äºå¤šå°‘", "assistant": "1+1ç­‰äº2"}
        ]

    è¿”å›:
        å¯¹è¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ {"user": ..., "assistant": ...} å­—å…¸
    """
    data_path = "data/sft_data.json"

    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)  # json.load ç›´æ¥è§£ææ–‡ä»¶ä¸º Python å¯¹è±¡
    else:
        print("æœªæ‰¾åˆ° SFT æ•°æ®æ–‡ä»¶")
        return []  # è¿”å›ç©ºåˆ—è¡¨è¡¨ç¤ºæ— æ•°æ®


def load_reward_data() -> List[Dict]:
    """
    åŠ è½½å¥–åŠ±æ¨¡å‹è®­ç»ƒæ•°æ®

    æ•°æ®æ ¼å¼ï¼šæ¯æ¡æ•°æ®åŒ…å«ä¸€ä¸ª prompt å’Œä¸¤ä¸ªå›ç­”ï¼ˆå¥½çš„å’Œå·®çš„ï¼‰
    ç¤ºä¾‹æ–‡ä»¶å†…å®¹ï¼š
        [
            {
                "prompt": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
                "chosen": "å»ºè®®ä» Python å¼€å§‹ï¼Œå®ƒè¯­æ³•ç®€æ´...",
                "rejected": "ç¼–ç¨‹å¾ˆéš¾å­¦"
            }
        ]

    è¿™ç§"åå¥½å¯¹æ¯”"æ•°æ®ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹åŒºåˆ†å›ç­”å¥½å

    è¿”å›:
        åå¥½æ•°æ®åˆ—è¡¨
    """
    data_path = "data/reward_data.json"

    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("æœªæ‰¾åˆ°å¥–åŠ±æ•°æ®æ–‡ä»¶")
        return []


def load_rlvf_data() -> List[Dict]:
    """
    åŠ è½½ RLVFï¼ˆå¯éªŒè¯åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰æ•°æ®

    æ•°æ®æ ¼å¼ï¼šæ•°å­¦æˆ–é€»è¾‘é—®é¢˜ï¼Œå¸¦æœ‰å¯éªŒè¯çš„æ­£ç¡®ç­”æ¡ˆ
    ç¤ºä¾‹æ–‡ä»¶å†…å®¹ï¼š
        [
            {"question": "2 + 3 = ?", "answer": "5"},
            {"question": "å¦‚æœ x = 2ï¼Œé‚£ä¹ˆ x * 3 = ?", "answer": "6"}
        ]

    RLVF çš„ç‰¹ç‚¹æ˜¯ç­”æ¡ˆå¯ä»¥è‡ªåŠ¨éªŒè¯ï¼ˆè€Œä¸éœ€è¦äººå·¥è¯„ä¼°ï¼‰

    è¿”å›:
        é—®ç­”æ•°æ®åˆ—è¡¨
    """
    data_path = "data/rlvf_data.json"

    if os.path.exists(data_path):
        with open(data_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("æœªæ‰¾åˆ° RLVF æ•°æ®æ–‡ä»¶")
        return []


def train_pretrain(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ

    ç›®æ ‡ï¼šå­¦ä¹ è¯­è¨€è§„å¾‹ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ (Pretrain)")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    corpus = load_pretrain_data()
    if not corpus:
        print("æ²¡æœ‰é¢„è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡")
        return {}

    dataset = PretrainDataset(corpus, tokenizer, seq_len=config.seq_len)
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0
    )

    # ============================================================
    # ä¼˜åŒ–å™¨ (Optimizer)
    # ============================================================
    # AdamW = Adam + Weight Decayï¼ˆæƒé‡è¡°å‡ï¼‰
    #
    # ä¸ºä»€ä¹ˆç”¨ AdamW è€Œä¸æ˜¯ SGDï¼Ÿ
    # - Adam è‡ªé€‚åº”è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡ï¼Œæ”¶æ•›æ›´å¿«
    # - Weight Decay æ˜¯æ­£åˆ™åŒ–æ‰‹æ®µï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    #
    # å‚æ•°è¯´æ˜ï¼š
    # - lr: å­¦ä¹ ç‡ï¼Œæ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•¿
    # - weight_decay: æƒé‡è¡°å‡ç³»æ•°ï¼Œç›¸å½“äº L2 æ­£åˆ™åŒ–
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.pretrain_lr,
        weight_decay=0.01
    )

    # ============================================================
    # å­¦ä¹ ç‡è°ƒåº¦å™¨ (Learning Rate Scheduler)
    # ============================================================
    # CosineAnnealingLR: ä½™å¼¦é€€ç«å­¦ä¹ ç‡
    #
    # å­¦ä¹ ç‡å˜åŒ–æ›²çº¿ï¼š
    #   lr
    #    â”‚
    #  maxâ”œâ”€â”€â”€â•®
    #    â”‚    â•²
    #    â”‚     â•²___
    #  minâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â•²____
    #    â”‚              â•²
    #    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â†’ epoch
    #    0               T_max
    #
    # ä¸ºä»€ä¹ˆè¦è¡°å‡å­¦ä¹ ç‡ï¼Ÿ
    # - è®­ç»ƒåˆæœŸï¼šå¤§å­¦ä¹ ç‡å¿«é€Ÿæ‰¾åˆ°å¥½çš„æ–¹å‘
    # - è®­ç»ƒåæœŸï¼šå°å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´ï¼Œé¿å…éœ‡è¡
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.pretrain_epochs
    )

    history = {'loss': []}

    # model.train() å¼€å¯è®­ç»ƒæ¨¡å¼
    # è¿™ä¼šå¯ç”¨ Dropout å’Œ BatchNorm çš„è®­ç»ƒè¡Œä¸º
    model.train()

    # ============================================================
    # è®­ç»ƒä¸»å¾ªç¯
    # ============================================================
    #
    # æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•´ä½“æµç¨‹ï¼š
    # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    # â”‚                    Epoch å¾ªç¯ï¼ˆéå†æ•´ä¸ªæ•°æ®é›†ï¼‰              â”‚
    # â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    # â”‚  â”‚               Batch å¾ªç¯ï¼ˆå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡ï¼‰               â”‚  â”‚
    # â”‚  â”‚                                                      â”‚  â”‚
    # â”‚  â”‚  1. å‰å‘ä¼ æ’­ â”€â”€â†’ è®¡ç®—é¢„æµ‹å€¼å’ŒæŸå¤±                      â”‚  â”‚
    # â”‚  â”‚  2. åå‘ä¼ æ’­ â”€â”€â†’ è®¡ç®—æ¢¯åº¦                             â”‚  â”‚
    # â”‚  â”‚  3. æ¢¯åº¦è£å‰ª â”€â”€â†’ é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸                         â”‚  â”‚
    # â”‚  â”‚  4. å‚æ•°æ›´æ–° â”€â”€â†’ æ ¹æ®æ¢¯åº¦è°ƒæ•´æƒé‡                      â”‚  â”‚
    # â”‚  â”‚                                                      â”‚  â”‚
    # â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    # â”‚  5. æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨                                        â”‚
    # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    for epoch in range(config.pretrain_epochs):
        total_loss = 0
        # tqdm æ˜¯è¿›åº¦æ¡åº“ï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        progress_bar = tqdm(dataloader, desc=f"Pretrain Epoch {epoch + 1}/{config.pretrain_epochs}")

        # ============================================================
        # Batch å¾ªç¯ï¼šéå†æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
        # ============================================================
        # dataloader æ¯æ¬¡è¿”å›ä¸€ä¸ª batch çš„æ•°æ®ï¼š
        #   input_ids:  [batch_size, seq_len]ï¼Œå¦‚ [16, 64]
        #   target_ids: [batch_size, seq_len]ï¼Œå¦‚ [16, 64]
        for input_ids, target_ids in progress_bar:
            # --------------------------------------------------------
            # Step 0: å°†æ•°æ®ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            # --------------------------------------------------------
            # .to(device) å°†å¼ é‡ä» CPU å¤åˆ¶åˆ° GPU
            # ç¥ç»ç½‘ç»œè®¡ç®—åœ¨ GPU ä¸Šæ¯” CPU å¿« 10-100 å€
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)

            # --------------------------------------------------------
            # Step 1: å‰å‘ä¼ æ’­ (Forward Pass)
            # --------------------------------------------------------
            # è¾“å…¥æ•°æ®é€šè¿‡æ¨¡å‹ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœå’ŒæŸå¤±
            #
            # æ•°æ®æµï¼š
            #   input_ids [16, 64]
            #       â†“
            #   Embedding Layerï¼ˆè¯å‘é‡ï¼‰
            #       â†“
            #   Transformer Blocks Ã— Nï¼ˆç‰¹å¾æå–ï¼‰
            #       â†“
            #   Output Layerï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰
            #       â†“
            #   logits [16, 64, vocab_size]
            #       â†“
            #   CrossEntropyLossï¼ˆä¸ target_ids æ¯”è¾ƒï¼‰
            #       â†“
            #   lossï¼ˆæ ‡é‡ï¼‰
            _, loss = model(input_ids, target_ids)

            # --------------------------------------------------------
            # Step 2: æ¸…é›¶æ¢¯åº¦
            # --------------------------------------------------------
            # ä¸ºä»€ä¹ˆè¦æ¸…é›¶ï¼Ÿ
            # PyTorch é»˜è®¤ä¼šç´¯åŠ æ¢¯åº¦ï¼Œå¦‚æœä¸æ¸…é›¶ï¼Œæ¢¯åº¦ä¼šè¶Šæ¥è¶Šå¤§
            # è¿™æ˜¯ä¸ºäº†æ”¯æŒ"æ¢¯åº¦ç´¯ç§¯"æŠ€æœ¯ï¼Œä½†é€šå¸¸æˆ‘ä»¬æ¯ä¸ª batch è¦æ¸…é›¶
            optimizer.zero_grad()

            # --------------------------------------------------------
            # Step 3: åå‘ä¼ æ’­ (Backward Pass)
            # --------------------------------------------------------
            # è®¡ç®—æŸå¤±å¯¹æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
            #
            # é“¾å¼æ³•åˆ™ï¼š
            #   âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚output Ã— âˆ‚output/âˆ‚hidden Ã— âˆ‚hidden/âˆ‚W
            #
            # loss.backward() è‡ªåŠ¨è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
            # æ¢¯åº¦å­˜å‚¨åœ¨ param.grad ä¸­
            loss.backward()

            # --------------------------------------------------------
            # Step 4: æ¢¯åº¦è£å‰ª (Gradient Clipping)
            # --------------------------------------------------------
            # é˜²æ­¢"æ¢¯åº¦çˆ†ç‚¸"é—®é¢˜
            #
            # ä»€ä¹ˆæ˜¯æ¢¯åº¦çˆ†ç‚¸ï¼Ÿ
            # - åœ¨æ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦é€šè¿‡é“¾å¼æ³•åˆ™ç›¸ä¹˜
            # - å¦‚æœæ¯å±‚æ¢¯åº¦ > 1ï¼Œç´¯ä¹˜åä¼šå˜å¾—éå¸¸å¤§
            # - å¯¼è‡´å‚æ•°æ›´æ–°è¿‡å¤§ï¼Œæ¨¡å‹å‘æ•£
            #
            # æ¢¯åº¦è£å‰ªåŸç†ï¼š
            #   å¦‚æœ ||gradients|| > max_norm:
            #       gradients = gradients Ã— (max_norm / ||gradients||)
            #
            # ä¸¾ä¾‹ï¼ˆmax_norm=1.0ï¼‰ï¼š
            #   åŸå§‹æ¢¯åº¦å‘é‡: [3, 4]ï¼ŒèŒƒæ•° = 5
            #   è£å‰ªå: [3, 4] Ã— (1/5) = [0.6, 0.8]ï¼ŒèŒƒæ•° = 1
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # --------------------------------------------------------
            # Step 5: å‚æ•°æ›´æ–°
            # --------------------------------------------------------
            # æ ¹æ®æ¢¯åº¦è°ƒæ•´æ¨¡å‹å‚æ•°
            #
            # Adam æ›´æ–°å…¬å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
            #   m = Î²1 Ã— m + (1-Î²1) Ã— grad      # ä¸€é˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡ï¼‰
            #   v = Î²2 Ã— v + (1-Î²2) Ã— gradÂ²     # äºŒé˜¶åŠ¨é‡ï¼ˆæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡ï¼‰
            #   param = param - lr Ã— m / (âˆšv + Îµ)
            #
            # æ¯ä¸ªå‚æ•°éƒ½ä¼šè¢«æ›´æ–°ï¼š
            #   W_new = W_old - lr Ã— âˆ‚L/âˆ‚W
            optimizer.step()

            # --------------------------------------------------------
            # ç»Ÿè®¡å’Œæ˜¾ç¤º
            # --------------------------------------------------------
            # .item() å°†å•å…ƒç´ å¼ é‡è½¬ä¸º Python æ•°å€¼
            total_loss += loss.item()
            # åœ¨è¿›åº¦æ¡å³ä¾§æ˜¾ç¤ºå½“å‰ batch çš„ loss
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        # ============================================================
        # Epoch ç»“æŸåçš„æ“ä½œ
        # ============================================================

        # æ›´æ–°å­¦ä¹ ç‡ï¼ˆæŒ‰ä½™å¼¦æ›²çº¿è¡°å‡ï¼‰
        scheduler.step()

        # è®¡ç®—å¹¶è®°å½•å¹³å‡æŸå¤±
        avg_loss = total_loss / len(dataloader)
        history['loss'].append(avg_loss)
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "pretrain_final.pt")
    torch.save(model.state_dict(), save_path)
    print(f"é¢„è®­ç»ƒæ¨¡å‹å·²ä¿å­˜: {save_path}")

    return history


def train_sft(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 2ï¼šç›‘ç£å¾®è°ƒ (SFT)

    ç›®æ ‡ï¼šå­¦ä¹ å¯¹è¯æ ¼å¼ï¼Œè·å¾—æŒ‡ä»¤éµå¾ªèƒ½åŠ›
    åŒ…å«æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 2ï¼šç›‘ç£å¾®è°ƒ (SFT)")
    print("=" * 60)

    # åŠ è½½æ•°æ®
    sft_data = load_sft_data()
    if not sft_data:
        print("æ²¡æœ‰ SFT æ•°æ®ï¼Œè·³è¿‡")
        return {}

    dataset = SFTDataset(sft_data, tokenizer, max_length=config.context_size)
    dataloader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn
    )

    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼‰
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.sft_lr,
        weight_decay=0.01
    )

    history = {'loss': []}
    model.train()

    # ============================================================
    # æ—©åœæœºåˆ¶ (Early Stopping) é…ç½®
    # ============================================================
    #
    # ä»€ä¹ˆæ˜¯æ—©åœï¼Ÿ
    # -----------
    # æ—©åœæ˜¯é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼šå½“æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„æ€§èƒ½ä¸å†æå‡æ—¶åœæ­¢è®­ç»ƒã€‚
    # åœ¨ SFT é˜¶æ®µç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºï¼š
    #   1. SFT æ•°æ®é›†é€šå¸¸è¾ƒå°ï¼ˆå‡ ç™¾åˆ°å‡ åƒæ¡ï¼‰
    #   2. æ¨¡å‹å¾ˆå®¹æ˜“åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ
    #   3. è¿‡æ‹Ÿåˆåæ¨¡å‹ä¼š"æ­»è®°ç¡¬èƒŒ"ï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™
    #
    # æˆ‘ä»¬çš„æ—©åœç­–ç•¥ï¼š
    # --------------
    #   1. åªæœ‰å½“ loss < min_loss_threshold æ—¶æ‰å¼€å§‹ç›‘æ§
    #      ï¼ˆé¿å…åœ¨è®­ç»ƒåˆæœŸè¯¯åˆ¤ï¼‰
    #   2. è¦æ±‚ loss æœ‰"æ˜æ˜¾æ”¹å–„"ï¼ˆä¸‹é™è¶…è¿‡ 0.01ï¼‰
    #   3. è¿ç»­ patience ä¸ª epoch æ²¡æœ‰æ˜æ˜¾æ”¹å–„å°±åœæ­¢
    #
    # å¯è§†åŒ–ï¼ˆå‡è®¾ min_loss_threshold=0.1, patience=5ï¼‰ï¼š
    #
    #   Loss
    #    â”‚
    #  1.0â”œâ”€â”€â”€â”€â•®
    #    â”‚    â•°â”€â”€â•®
    #  0.5â”œ       â•°â”€â”€â”€â•®
    #    â”‚           â•°â”€â”€â•®
    #  0.1â”œâ”€ â”€ â”€ â”€ â”€ â”€ â”€â•°â”€â”€â•®â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† å¼€å§‹ç›‘æ§
    #    â”‚                 â•°â”€â•® â•­â”€â•® â•­â”€â•®
    #  0.05â”œ                  â•°â”€â•¯ â•°â”€â•¯ â•°â”€â†’  â† 5æ¬¡æ— æ”¹å–„ï¼Œåœæ­¢
    #    â”‚
    #    â””â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â†’ Epoch
    #        1   2   3   4   5   6   7   8

    best_loss = float('inf')         # è®°å½•æœ€ä½³ loss
    patience = 5                      # å®¹å¿æ¬¡æ•°ï¼šè¿ç»­å‡ æ¬¡æ— æ”¹å–„ååœæ­¢
    patience_counter = 0              # å½“å‰å·²è¿ç»­æ— æ”¹å–„çš„æ¬¡æ•°
    min_loss_threshold = 0.1          # å¼€å§‹ç›‘æ§çš„é˜ˆå€¼ï¼ˆloss ä½äºæ­¤å€¼æ‰å¼€å§‹ï¼‰
    improvement_threshold = 0.01      # æ”¹å–„é˜ˆå€¼ï¼šloss éœ€è¦ä¸‹é™è¶…è¿‡æ­¤å€¼æ‰ç®—"æ”¹å–„"
    #
    # ä¸ºä»€ä¹ˆ improvement_threshold = 0.01ï¼Ÿ
    # -----------------------------------
    # 1. å¤ªå°ï¼ˆå¦‚ 0.001ï¼‰ï¼šå¯¹å™ªå£°è¿‡äºæ•æ„Ÿï¼Œå¯èƒ½è¿‡æ—©åœæ­¢
    # 2. å¤ªå¤§ï¼ˆå¦‚ 0.1ï¼‰ï¼šå¯èƒ½é”™è¿‡æœ€ä½³ç‚¹ï¼Œè®­ç»ƒè¿‡ä¹…
    # 3. 0.01 æ˜¯ç»éªŒå€¼ï¼šåœ¨å¤§å¤šæ•° SFT ä»»åŠ¡ä¸­æ•ˆæœè‰¯å¥½
    #    - å¯¹äº loss åœ¨ 0.01-0.1 èŒƒå›´å†…ï¼Œ0.01 çš„å˜åŒ–çº¦æ˜¯ 10%-100%
    #    - è¿™ä¸ªå¹…åº¦è¶³å¤ŸåŒºåˆ†çœŸæ­£çš„æ”¹å–„å’Œéšæœºæ³¢åŠ¨

    for epoch in range(config.sft_epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"SFT Epoch {epoch + 1}/{config.sft_epochs}")

        for input_ids, target_ids in progress_bar:
            input_ids = input_ids.to(device)
            target_ids = target_ids.to(device)

            _, loss = model(input_ids, target_ids)

            optimizer.zero_grad()
            loss.backward()
            # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œ1.0 æ˜¯å¸¸ç”¨çš„é˜ˆå€¼
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_loss = total_loss / len(dataloader)
        history['loss'].append(avg_loss)
        print(f"Epoch {epoch + 1} - Loss: {avg_loss:.4f}")

        # ============================================================
        # æ—©åœæ£€æŸ¥é€»è¾‘
        # ============================================================
        # æ¡ä»¶1ï¼šåªæœ‰ loss ä½äºé˜ˆå€¼æ‰å¼€å§‹ç›‘æ§
        # è¿™æ˜¯å› ä¸ºè®­ç»ƒåˆæœŸ loss æ³¢åŠ¨è¾ƒå¤§ï¼Œä¸é€‚åˆåšæ—©åœåˆ¤æ–­
        if avg_loss < min_loss_threshold:

            # æ¡ä»¶2ï¼šæ£€æŸ¥æ˜¯å¦æœ‰"æ˜æ˜¾æ”¹å–„"
            # best_loss - 0.01 è¡¨ç¤º loss éœ€è¦æ¯”å†å²æœ€ä½³ä½è‡³å°‘ 0.01
            if avg_loss < best_loss - improvement_threshold:
                # æœ‰æ˜æ˜¾æ”¹å–„ï¼šæ›´æ–°æœ€ä½³è®°å½•ï¼Œé‡ç½®è®¡æ•°å™¨
                best_loss = avg_loss
                patience_counter = 0
                # ä¿å­˜å½“å‰æœ€ä½³æ¨¡å‹ï¼ˆä»¥ä¾¿åç»­æ¢å¤ï¼‰
                best_path = os.path.join(config.checkpoint_dir, "sft_best.pt")
                torch.save(model.state_dict(), best_path)
            else:
                # æ— æ˜æ˜¾æ”¹å–„ï¼šå¢åŠ è®¡æ•°å™¨
                patience_counter += 1
                print(f"  âš ï¸ Loss æ”¹å–„ä¸æ˜æ˜¾ ({patience_counter}/{patience})")

            # æ¡ä»¶3ï¼šè¿ç»­ patience æ¬¡æ— æ”¹å–„ï¼Œè§¦å‘æ—©åœ
            if patience_counter >= patience:
                print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {patience} ä¸ª epoch æ²¡æœ‰æ˜æ˜¾æ”¹å–„")
                print(f"   æœ€ä½³ Loss: {best_loss:.4f}")
                # æ¢å¤åˆ°æœ€ä½³æ¨¡å‹ï¼ˆé¿å…ä½¿ç”¨è¿‡æ‹Ÿåˆçš„æœ€åä¸€ä¸ªç‰ˆæœ¬ï¼‰
                best_path = os.path.join(config.checkpoint_dir, "sft_best.pt")
                if os.path.exists(best_path):
                    model.load_state_dict(torch.load(best_path, map_location=device, weights_only=True))
                break

    # ä¿å­˜æ¨¡å‹
    save_path = os.path.join(config.checkpoint_dir, "sft_final.pt")
    torch.save(model.state_dict(), save_path)
    print(f"SFT æ¨¡å‹å·²ä¿å­˜: {save_path}")

    return history


def train_reward_model(
    base_model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
):
    """
    é˜¶æ®µ 3ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)

    ç›®æ ‡ï¼šå­¦ä¹ äººç±»åå¥½ï¼Œèƒ½å¤Ÿç»™å›ç­”æ‰“åˆ†

    å¥–åŠ±æ¨¡å‹çš„ä½œç”¨ï¼š
    ---------------
    åœ¨ RLHF ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª"è£åˆ¤"æ¥è¯„ä»·æ¨¡å‹ç”Ÿæˆçš„å›ç­”å¥½ä¸å¥½ã€‚
    è¿™ä¸ªè£åˆ¤å°±æ˜¯å¥–åŠ±æ¨¡å‹ã€‚

    å·¥ä½œåŸç†ï¼š
    ---------
    1. è¾“å…¥ï¼š(prompt + response) çš„å®Œæ•´å¯¹è¯
    2. è¾“å‡ºï¼šä¸€ä¸ªæ ‡é‡åˆ†æ•°ï¼Œè¡¨ç¤ºå›ç­”çš„è´¨é‡
    3. è®­ç»ƒæ•°æ®ï¼šäººç±»æ ‡æ³¨çš„åå¥½å¯¹ (chosen > rejected)

    è®­ç»ƒæµç¨‹ï¼š
    ---------
    1. ä» SFT æ¨¡å‹åˆå§‹åŒ–ï¼ˆå…±äº« Transformer æƒé‡ï¼‰
    2. æ›¿æ¢è¾“å‡ºå±‚ä¸ºæ ‡é‡å¥–åŠ±å¤´
    3. ä½¿ç”¨ Bradley-Terry æŸå¤±è®­ç»ƒ

    å‚æ•°:
        base_model: åŸºç¡€ GPT æ¨¡å‹ï¼ˆç”¨äºåˆå§‹åŒ–ï¼‰
        tokenizer: åˆ†è¯å™¨
        config: è®­ç»ƒé…ç½®
        device: è®¡ç®—è®¾å¤‡

    è¿”å›:
        è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰æ•°æ®åˆ™è¿”å› None
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 3ï¼šè®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)")
    print("=" * 60)

    # ============================================================
    # Step 1: åŠ è½½åå¥½æ•°æ®
    # ============================================================
    reward_data = load_reward_data()
    if not reward_data:
        print("æ²¡æœ‰å¥–åŠ±æ•°æ®ï¼Œè·³è¿‡")
        return None  # è¿”å› None è¡¨ç¤ºè·³è¿‡

    # ============================================================
    # Step 2: å¯¼å…¥å¥–åŠ±æ¨¡å‹ç›¸å…³ç±»
    # ============================================================
    # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
    from reward_model import RewardModel, RewardModelTrainer

    # ============================================================
    # Step 3: åˆ›å»ºæ¨¡å‹é…ç½®
    # ============================================================
    # å¥–åŠ±æ¨¡å‹æ¶æ„ä¸åŸºç¡€æ¨¡å‹ç›¸åŒï¼ˆå…±äº« Transformer ç»“æ„ï¼‰
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),  # è¯è¡¨å¤§å°
        emb_dim=config.emb_dim,           # åµŒå…¥ç»´åº¦
        num_heads=config.num_heads,       # æ³¨æ„åŠ›å¤´æ•°
        num_layers=config.num_layers,     # Transformer å±‚æ•°
        context_size=config.context_size, # ä¸Šä¸‹æ–‡é•¿åº¦
        dropout=config.dropout            # Dropout æ¯”ä¾‹
    )

    # ============================================================
    # Step 4: ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
    # ============================================================
    # è¿™æ ·å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒå­¦åˆ°çš„è¯­è¨€çŸ¥è¯†ï¼ŒåŠ é€Ÿå¥–åŠ±æ¨¡å‹è®­ç»ƒ
    reward_model = RewardModel.from_pretrained(base_model, model_config)

    # ============================================================
    # Step 5: åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    # ============================================================
    trainer = RewardModelTrainer(
        reward_model,
        tokenizer,
        model_config,
        learning_rate=config.reward_lr,      # å¥–åŠ±æ¨¡å‹å­¦ä¹ ç‡
        num_epochs=config.reward_epochs      # è®­ç»ƒè½®æ•°
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train(reward_data, batch_size=config.reward_batch_size)

    # ============================================================
    # Step 6: ä¿å­˜æ¨¡å‹
    # ============================================================
    save_path = os.path.join(config.checkpoint_dir, "reward_model.pt")
    trainer.save_model(save_path)

    return reward_model


def train_rlhf(
    model: GPT,
    reward_model,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 4ï¼šRLHF (PPO) è®­ç»ƒ

    ç›®æ ‡ï¼šåˆ©ç”¨å¥–åŠ±æ¨¡å‹æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼Œè®©æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å›ç­”

    RLHF æ˜¯ä»€ä¹ˆï¼Ÿ
    ------------
    RLHF = Reinforcement Learning from Human Feedback
    å³"åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ "

    æ ¸å¿ƒæ€æƒ³ï¼š
    ---------
    1. è®©æ¨¡å‹ç”Ÿæˆå›ç­”ï¼ˆé‡‡æ ·ï¼‰
    2. ç”¨å¥–åŠ±æ¨¡å‹ç»™å›ç­”æ‰“åˆ†ï¼ˆè·å–å¥–åŠ±ï¼‰
    3. æ ¹æ®åˆ†æ•°ä¼˜åŒ–æ¨¡å‹ï¼ˆPPO ç®—æ³•ï¼‰
    4. é‡å¤ä»¥ä¸Šæ­¥éª¤

    PPO ç®—æ³•ç®€ä»‹ï¼š
    -------------
    PPO = Proximal Policy Optimizationï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰
    - é™åˆ¶æ¯æ¬¡æ›´æ–°çš„å¹…åº¦ï¼Œé˜²æ­¢ç­–ç•¥å‰§çƒˆå˜åŒ–
    - ä½¿ç”¨ clip æœºåˆ¶ç¡®ä¿ç¨³å®šæ€§
    - æ˜¯ç›®å‰ RLHF ä¸­æœ€å¸¸ç”¨çš„ RL ç®—æ³•

    è®­ç»ƒå¾ªç¯ï¼š
    ---------
    for each episode:
        1. éšæœºé€‰æ‹©ä¸€ä¸ª prompt
        2. è®©æ¨¡å‹ç”Ÿæˆ response
        3. ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®— reward
        4. è®¡ç®— PPO æŸå¤±å¹¶æ›´æ–°æ¨¡å‹

    å‚æ•°:
        model: ç­–ç•¥æ¨¡å‹ï¼ˆè¦ä¼˜åŒ–çš„ GPT æ¨¡å‹ï¼‰
        reward_model: å¥–åŠ±æ¨¡å‹ï¼ˆè¯„åˆ†å™¨ï¼‰
        tokenizer: åˆ†è¯å™¨
        config: è®­ç»ƒé…ç½®
        device: è®¡ç®—è®¾å¤‡

    è¿”å›:
        è®­ç»ƒå†å²è®°å½•
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 4ï¼šRLHF (PPO) è®­ç»ƒ")
    print("=" * 60)

    # ============================================================
    # Step 1: æ£€æŸ¥å¥–åŠ±æ¨¡å‹æ˜¯å¦å¯ç”¨
    # ============================================================
    if reward_model is None:
        print("æ²¡æœ‰å¥–åŠ±æ¨¡å‹ï¼Œè·³è¿‡ RLHF")
        return {}  # æ²¡æœ‰å¥–åŠ±æ¨¡å‹æ— æ³•è¿›è¡Œ RLHF

    # ============================================================
    # Step 2: å¯¼å…¥ PPO è®­ç»ƒå™¨
    # ============================================================
    from rlhf import PPOTrainer, RLHFConfig

    # ============================================================
    # Step 3: è·å–è®­ç»ƒæç¤ºï¼ˆpromptsï¼‰
    # ============================================================
    # RLHF éœ€è¦ prompt æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆå›ç­”
    # æˆ‘ä»¬ä» SFT æ•°æ®ä¸­æå– user é—®é¢˜ä½œä¸º prompt
    sft_data = load_sft_data()
    if not sft_data:
        print("æ²¡æœ‰ SFT æ•°æ®æä¾›æç¤ºï¼Œè·³è¿‡ RLHF")
        return {}

    # æå–æ‰€æœ‰ç”¨æˆ·é—®é¢˜ä½œä¸º prompts
    prompts = [item['user'] for item in sft_data]

    # ============================================================
    # Step 4: åˆ›å»ºæ¨¡å‹é…ç½®
    # ============================================================
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),
        emb_dim=config.emb_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        context_size=config.context_size,
        dropout=0.0  # æ¨ç†/ç”Ÿæˆæ—¶ä¸ä½¿ç”¨ dropoutï¼ˆç¡®ä¿ä¸€è‡´æ€§ï¼‰
    )

    # ============================================================
    # Step 5: é…ç½® RLHF è¶…å‚æ•°
    # ============================================================
    rlhf_config = RLHFConfig(
        clip_ratio=0.2,              # PPO è£å‰ªæ¯”ä¾‹ï¼šé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦
        kl_coef=0.01,                # KL æ•£åº¦ç³»æ•°ï¼šæƒ©ç½šåç¦»åŸç­–ç•¥å¤ªè¿œ
        learning_rate=config.rlhf_lr,  # å­¦ä¹ ç‡
        num_episodes=config.rlhf_episodes,  # è®­ç»ƒè½®æ•°
        batch_size=config.rlhf_batch_size,  # æ‰¹æ¬¡å¤§å°
        max_new_tokens=64            # ç”Ÿæˆçš„æœ€å¤§ token æ•°
    )

    # ============================================================
    # Step 6: åˆ›å»º PPO è®­ç»ƒå™¨
    # ============================================================
    trainer = PPOTrainer(
        policy_model=model,          # ç­–ç•¥æ¨¡å‹ï¼ˆè¦ä¼˜åŒ–çš„æ¨¡å‹ï¼‰
        reward_model=reward_model,   # å¥–åŠ±æ¨¡å‹ï¼ˆè¯„åˆ†å™¨ï¼‰
        tokenizer=tokenizer,
        config=model_config,
        rlhf_config=rlhf_config
    )

    # ============================================================
    # Step 7: å¼€å§‹è®­ç»ƒ
    # ============================================================
    history = trainer.train(prompts)

    # ============================================================
    # Step 8: ä¿å­˜æ¨¡å‹
    # ============================================================
    save_path = os.path.join(config.checkpoint_dir, "rlhf_final.pt")
    trainer.save_model(save_path)

    return history


def train_rlvf(
    model: GPT,
    tokenizer: BPETokenizer,
    config: argparse.Namespace,
    device: torch.device
) -> Dict:
    """
    é˜¶æ®µ 5ï¼šRLVF è®­ç»ƒ

    ç›®æ ‡ï¼šåˆ©ç”¨å¯éªŒè¯åé¦ˆæå‡ç²¾ç¡®æ¨ç†èƒ½åŠ›

    RLVF æ˜¯ä»€ä¹ˆï¼Ÿ
    ------------
    RLVF = Reinforcement Learning from Verifiable Feedback
    å³"åŸºäºå¯éªŒè¯åé¦ˆçš„å¼ºåŒ–å­¦ä¹ "

    ä¸ RLHF çš„åŒºåˆ«ï¼š
    ---------------
    | ç‰¹æ€§     | RLHF              | RLVF              |
    |----------|-------------------|-------------------|
    | åé¦ˆæ¥æº | äººç±»åå¥½/å¥–åŠ±æ¨¡å‹  | è‡ªåŠ¨éªŒè¯å™¨        |
    | é€‚ç”¨åœºæ™¯ | å¼€æ”¾å¼å›ç­”         | æœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜   |
    | æˆæœ¬     | éœ€è¦äººç±»æ ‡æ³¨       | è‡ªåŠ¨åŒ–ï¼Œæˆæœ¬ä½     |
    | å‡†ç¡®æ€§   | ä¸»è§‚ï¼Œå¯èƒ½æœ‰å™ªå£°   | å®¢è§‚ï¼Œ100%å‡†ç¡®     |

    RLVF çš„ä¼˜åŠ¿ï¼š
    -------------
    1. æ•°å­¦é¢˜ï¼šç­”æ¡ˆå¯ä»¥è‡ªåŠ¨éªŒè¯ (2+3=5 âœ“)
    2. ä»£ç é¢˜ï¼šå¯ä»¥è¿è¡Œæµ‹è¯•éªŒè¯
    3. é€»è¾‘é¢˜ï¼šå¯ä»¥å½¢å¼åŒ–éªŒè¯
    4. æ— éœ€å¥–åŠ±æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨æ­£ç¡®/é”™è¯¯ä½œä¸ºå¥–åŠ±

    è®­ç»ƒæµç¨‹ï¼š
    ---------
    for each iteration:
        1. é€‰æ‹©ä¸€ä¸ªæ•°å­¦/é€»è¾‘é—®é¢˜
        2. è®©æ¨¡å‹ç”Ÿæˆå¤šä¸ªç­”æ¡ˆ
        3. éªŒè¯ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
        4. æ­£ç¡®çš„ç»™æ­£å¥–åŠ±ï¼Œé”™è¯¯çš„ç»™è´Ÿå¥–åŠ±
        5. ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ›´æ–°æ¨¡å‹

    å‚æ•°:
        model: ç­–ç•¥æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        config: è®­ç»ƒé…ç½®
        device: è®¡ç®—è®¾å¤‡

    è¿”å›:
        è®­ç»ƒå†å²è®°å½•
    """
    print("\n" + "=" * 60)
    print("é˜¶æ®µ 5ï¼šRLVF è®­ç»ƒ")
    print("=" * 60)

    # ============================================================
    # Step 1: åŠ è½½ RLVF æ•°æ®ï¼ˆæ•°å­¦/é€»è¾‘é—®é¢˜ï¼‰
    # ============================================================
    rlvf_data = load_rlvf_data()
    if not rlvf_data:
        print("æ²¡æœ‰ RLVF æ•°æ®ï¼Œè·³è¿‡")
        return {}

    # ============================================================
    # Step 2: å¯¼å…¥ RLVF è®­ç»ƒå™¨
    # ============================================================
    from rlvf import RLVFTrainer, RLVFConfig

    # ============================================================
    # Step 3: åˆ›å»ºæ¨¡å‹é…ç½®
    # ============================================================
    model_config = MyLLMConfig(
        vocab_size=len(tokenizer.vocab),
        emb_dim=config.emb_dim,
        num_heads=config.num_heads,
        num_layers=config.num_layers,
        context_size=config.context_size,
        dropout=0.0  # ç”Ÿæˆæ—¶å…³é—­ dropout
    )

    # ============================================================
    # Step 4: é…ç½® RLVF è¶…å‚æ•°
    # ============================================================
    rlvf_config = RLVFConfig(
        num_iterations=config.rlvf_iterations,  # è®­ç»ƒè¿­ä»£æ¬¡æ•°
        samples_per_task=2,           # æ¯ä¸ªé—®é¢˜ç”Ÿæˆå‡ ä¸ªç­”æ¡ˆ
        correct_reward=1.0,           # ç­”å¯¹æ—¶çš„å¥–åŠ±
        incorrect_reward=-0.5,        # ç­”é”™æ—¶çš„æƒ©ç½šï¼ˆè´Ÿå¥–åŠ±ï¼‰
        learning_rate=config.rlvf_lr, # å­¦ä¹ ç‡
        max_new_tokens=32             # ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼ˆç­”æ¡ˆé€šå¸¸å¾ˆçŸ­ï¼‰
    )

    # ============================================================
    # Step 5: åˆ›å»º RLVF è®­ç»ƒå™¨
    # ============================================================
    trainer = RLVFTrainer(
        policy_model=model,
        tokenizer=tokenizer,
        config=model_config,
        rlvf_config=rlvf_config
    )

    # ============================================================
    # Step 6: å¼€å§‹è®­ç»ƒ
    # ============================================================
    history = trainer.train(rlvf_data, batch_size=config.rlvf_batch_size)

    # ============================================================
    # Step 7: ä¿å­˜æ¨¡å‹
    # ============================================================
    save_path = os.path.join(config.checkpoint_dir, "rlvf_final.pt")
    trainer.save_model(save_path)

    return history


# ==========================================
# ä¸»å‡½æ•°
# ==========================================
# ç¨‹åºå…¥å£ç‚¹ï¼Œè´Ÿè´£ï¼š
# 1. è§£æå‘½ä»¤è¡Œå‚æ•°
# 2. åˆå§‹åŒ–åˆ†è¯å™¨å’Œæ¨¡å‹
# 3. æŒ‰é¡ºåºæ‰§è¡Œ 5 ä¸ªè®­ç»ƒé˜¶æ®µ
# 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„ 5 é˜¶æ®µè®­ç»ƒæµç¨‹

    5 é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š
    ===============
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  é˜¶æ®µ 1: Pretrainï¼ˆé¢„è®­ç»ƒï¼‰                                  â”‚
    â”‚  â”œâ”€â”€ ç›®æ ‡ï¼šå­¦ä¹ è¯­è¨€è§„å¾‹                                      â”‚
    â”‚  â”œâ”€â”€ æ•°æ®ï¼šå¤§é‡æ— æ ‡æ³¨æ–‡æœ¬                                    â”‚
    â”‚  â””â”€â”€ è¾“å‡ºï¼špretrain_final.pt                                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  é˜¶æ®µ 2: SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰                                     â”‚
    â”‚  â”œâ”€â”€ ç›®æ ‡ï¼šå­¦ä¹ å¯¹è¯æ ¼å¼å’ŒæŒ‡ä»¤éµå¾ª                            â”‚
    â”‚  â”œâ”€â”€ æ•°æ®ï¼šäººå·¥æ ‡æ³¨çš„å¯¹è¯æ•°æ®                                â”‚
    â”‚  â””â”€â”€ è¾“å‡ºï¼šsft_final.pt                                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  é˜¶æ®µ 3: Reward Modelï¼ˆå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰                        â”‚
    â”‚  â”œâ”€â”€ ç›®æ ‡ï¼šå­¦ä¹ äººç±»åå¥½ï¼Œç»™å›ç­”æ‰“åˆ†                          â”‚
    â”‚  â”œâ”€â”€ æ•°æ®ï¼šåå¥½å¯¹æ¯”æ•°æ® (chosen vs rejected)                 â”‚
    â”‚  â””â”€â”€ è¾“å‡ºï¼šreward_model.pt                                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  é˜¶æ®µ 4: RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰                      â”‚
    â”‚  â”œâ”€â”€ ç›®æ ‡ï¼šè®©æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å›ç­”                    â”‚
    â”‚  â”œâ”€â”€ æ–¹æ³•ï¼šPPO ç®—æ³• + å¥–åŠ±æ¨¡å‹                               â”‚
    â”‚  â””â”€â”€ è¾“å‡ºï¼šrlhf_final.pt                                    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  é˜¶æ®µ 5: RLVFï¼ˆå¯éªŒè¯åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰                          â”‚
    â”‚  â”œâ”€â”€ ç›®æ ‡ï¼šæå‡ç²¾ç¡®æ¨ç†èƒ½åŠ›ï¼ˆæ•°å­¦ã€é€»è¾‘ï¼‰                    â”‚
    â”‚  â”œâ”€â”€ æ–¹æ³•ï¼šè‡ªåŠ¨éªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§ä½œä¸ºå¥–åŠ±                        â”‚
    â”‚  â””â”€â”€ è¾“å‡ºï¼šrlvf_final.pt                                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ---------
    # å®Œæ•´è®­ç»ƒï¼ˆä»å¤´å¼€å§‹ï¼‰
    python train.py

    # è·³è¿‡é¢„è®­ç»ƒï¼ˆä½¿ç”¨å·²æœ‰æ¨¡å‹ï¼‰
    python train.py --skip-pretrain

    # åªè®­ç»ƒ SFTï¼ˆè·³è¿‡å…¶ä»–é˜¶æ®µï¼‰
    python train.py --skip-pretrain --skip-reward --skip-rlhf --skip-rlvf

    # è‡ªå®šä¹‰å‚æ•°
    python train.py --emb_dim 512 --num_layers 6 --pretrain_epochs 20
    """

    # ============================================================
    # Step 1: åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    # ============================================================
    # argparse æ˜¯ Python æ ‡å‡†åº“ï¼Œç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="MyLLM å®Œæ•´ 5 é˜¶æ®µè®­ç»ƒ")

    # ------------------------------------------------------------
    # é˜¶æ®µæ§åˆ¶å‚æ•°ï¼šå†³å®šè·³è¿‡å“ªäº›è®­ç»ƒé˜¶æ®µ
    # ------------------------------------------------------------
    # action="store_true" è¡¨ç¤ºï¼šå¦‚æœæä¾›äº†è¿™ä¸ªå‚æ•°ï¼Œå€¼ä¸º Trueï¼›å¦åˆ™ä¸º False
    parser.add_argument("--skip-pretrain", action="store_true", help="è·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")
    parser.add_argument("--skip-sft", action="store_true", help="è·³è¿‡ SFT é˜¶æ®µ")
    parser.add_argument("--skip-reward", action="store_true", help="è·³è¿‡å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
    parser.add_argument("--skip-rlhf", action="store_true", help="è·³è¿‡ RLHF é˜¶æ®µ")
    parser.add_argument("--skip-rlvf", action="store_true", help="è·³è¿‡ RLVF é˜¶æ®µ")

    # ------------------------------------------------------------
    # æ¨¡å‹æ¶æ„å‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--vocab_size", type=int, default=2000,
                        help="è¯è¡¨å¤§å°ï¼šæ¨¡å‹èƒ½è¯†åˆ«çš„ä¸åŒ token æ•°é‡")
    parser.add_argument("--emb_dim", type=int, default=256,
                        help="åµŒå…¥ç»´åº¦ï¼šæ¯ä¸ª token çš„å‘é‡è¡¨ç¤ºç»´åº¦")
    parser.add_argument("--num_heads", type=int, default=4,
                        help="æ³¨æ„åŠ›å¤´æ•°ï¼šå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°")
    parser.add_argument("--num_layers", type=int, default=4,
                        help="Transformer å±‚æ•°ï¼šæ¨¡å‹æ·±åº¦")
    parser.add_argument("--context_size", type=int, default=256,
                        help="ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§åºåˆ—é•¿åº¦")
    parser.add_argument("--dropout", type=float, default=0.1,
                        help="Dropout æ¯”ä¾‹ï¼šéšæœºä¸¢å¼ƒç¥ç»å…ƒçš„æ¦‚ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")

    # ------------------------------------------------------------
    # é€šç”¨è®­ç»ƒå‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--batch_size", type=int, default=16,
                        help="æ‰¹æ¬¡å¤§å°ï¼šæ¯æ¬¡æ›´æ–°ä½¿ç”¨çš„æ ·æœ¬æ•°")
    parser.add_argument("--seq_len", type=int, default=64,
                        help="åºåˆ—é•¿åº¦ï¼šé¢„è®­ç»ƒæ—¶æ¯ä¸ªæ ·æœ¬çš„ token æ•°")

    # ------------------------------------------------------------
    # é¢„è®­ç»ƒå‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--pretrain_epochs", type=int, default=10,
                        help="é¢„è®­ç»ƒè½®æ•°ï¼šéå†æ•´ä¸ªæ•°æ®é›†çš„æ¬¡æ•°")
    parser.add_argument("--pretrain_lr", type=float, default=3e-4,
                        help="é¢„è®­ç»ƒå­¦ä¹ ç‡ï¼š3e-4 = 0.0003ï¼Œæ˜¯å¸¸ç”¨çš„åˆå§‹å­¦ä¹ ç‡")

    # ------------------------------------------------------------
    # SFT å‚æ•°
    # ------------------------------------------------------------
    # æ³¨æ„ï¼šepoch è¿‡å¤šä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼å»ºè®® 15-30
    parser.add_argument("--sft_epochs", type=int, default=20,
                        help="SFT è®­ç»ƒè½®æ•°ï¼ˆå»ºè®® 15-30ï¼Œè¿‡å¤šä¼šè¿‡æ‹Ÿåˆï¼‰")
    parser.add_argument("--sft_lr", type=float, default=5e-5,
                        help="SFT å­¦ä¹ ç‡ï¼šæ¯”é¢„è®­ç»ƒå°ï¼Œé¿å…ç ´åé¢„è®­ç»ƒçŸ¥è¯†")

    # ------------------------------------------------------------
    # å¥–åŠ±æ¨¡å‹å‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--reward_epochs", type=int, default=15,
                        help="å¥–åŠ±æ¨¡å‹è®­ç»ƒè½®æ•°ï¼šéœ€è¦è¶³å¤Ÿè½®æ¬¡å­¦ä¹ åå¥½")
    parser.add_argument("--reward_lr", type=float, default=1e-5,
                        help="å¥–åŠ±æ¨¡å‹å­¦ä¹ ç‡ï¼šè¾ƒå°çš„å­¦ä¹ ç‡ç¡®ä¿ç¨³å®š")
    parser.add_argument("--reward_batch_size", type=int, default=4,
                        help="å¥–åŠ±æ¨¡å‹æ‰¹æ¬¡å¤§å°ï¼šåå¥½å¯¹æ¯”éœ€è¦æˆå¯¹æ•°æ®")

    # ------------------------------------------------------------
    # RLHF å‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--rlhf_episodes", type=int, default=100,
                        help="RLHF è®­ç»ƒè½®æ•°ï¼šå¼ºåŒ–å­¦ä¹ éœ€è¦æ›´å¤šè¿­ä»£")
    parser.add_argument("--rlhf_lr", type=float, default=1e-5,
                        help="RLHF å­¦ä¹ ç‡ï¼šRL éœ€è¦å°å­¦ä¹ ç‡ä¿æŒç¨³å®š")
    parser.add_argument("--rlhf_batch_size", type=int, default=4,
                        help="RLHF æ‰¹æ¬¡å¤§å°ï¼šç”Ÿæˆ+è¯„ä¼°çš„å¼€é”€å¤§ï¼Œæ‰¹æ¬¡è¾ƒå°")

    # ------------------------------------------------------------
    # RLVF å‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--rlvf_iterations", type=int, default=60,
                        help="RLVF è¿­ä»£æ¬¡æ•°ï¼šæ¯æ¬¡è¿­ä»£å¤„ç†ä¸€æ‰¹é—®é¢˜")
    parser.add_argument("--rlvf_lr", type=float, default=1e-5,
                        help="RLVF å­¦ä¹ ç‡")
    parser.add_argument("--rlvf_batch_size", type=int, default=4,
                        help="RLVF æ‰¹æ¬¡å¤§å°")

    # ------------------------------------------------------------
    # æ–‡ä»¶è·¯å¾„å‚æ•°
    # ------------------------------------------------------------
    parser.add_argument("--checkpoint_dir", type=str, default="checkpoints",
                        help="æ£€æŸ¥ç‚¹ç›®å½•ï¼šæ¨¡å‹ä¿å­˜ä½ç½®")
    parser.add_argument("--vocab_path", type=str, default="checkpoints/vocab.json",
                        help="è¯è¡¨è·¯å¾„ï¼šåˆ†è¯å™¨ä¿å­˜/åŠ è½½ä½ç½®")

    # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œè¿”å› Namespace å¯¹è±¡
    # å¯ä»¥ç”¨ args.å‚æ•°å è®¿é—®å„ä¸ªå‚æ•°å€¼
    args = parser.parse_args()

    # ============================================================
    # Step 2: è®¾ç½®è®¡ç®—è®¾å¤‡
    # ============================================================
    # torch.cuda.is_available() æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
    # å¦‚æœæœ‰ GPUï¼Œä½¿ç”¨ "cuda"ï¼›å¦åˆ™ä½¿ç”¨ "cpu"
    # GPU è®­ç»ƒé€Ÿåº¦é€šå¸¸æ¯” CPU å¿« 10-100 å€
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ============================================================
    # Step 3: åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
    # ============================================================
    # exist_ok=True è¡¨ç¤ºå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™
    os.makedirs(args.checkpoint_dir, exist_ok=True)

    # ==========================================
    # Step 4: å‡†å¤‡åˆ†è¯å™¨
    # ==========================================
    # åˆ†è¯å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½ç†è§£çš„æ•°å­—åºåˆ—
    # å¦‚æœå·²æœ‰è®­ç»ƒå¥½çš„åˆ†è¯å™¨ï¼Œç›´æ¥åŠ è½½ï¼›å¦åˆ™ä»è¯­æ–™è®­ç»ƒæ–°çš„
    print("\n" + "=" * 60)
    print("å‡†å¤‡åˆ†è¯å™¨")
    print("=" * 60)

    if os.path.exists(args.vocab_path):
        # åŠ è½½å·²æœ‰åˆ†è¯å™¨ï¼ˆåŒ…å«è¯è¡¨å’Œåˆå¹¶è§„åˆ™ï¼‰
        print(f"åŠ è½½å·²æœ‰åˆ†è¯å™¨: {args.vocab_path}")
        tokenizer = BPETokenizer.load(args.vocab_path)
    else:
        # è®­ç»ƒæ–°åˆ†è¯å™¨
        print("è®­ç»ƒæ–°åˆ†è¯å™¨...")
        corpus = load_pretrain_data()              # åŠ è½½è®­ç»ƒè¯­æ–™
        tokenizer = BPETokenizer(vocab_size=args.vocab_size)  # åˆ›å»ºåˆ†è¯å™¨
        tokenizer.fit(corpus, verbose=True)        # è®­ç»ƒ BPE åˆå¹¶è§„åˆ™
        tokenizer.save(args.vocab_path)            # ä¿å­˜åˆ°æ–‡ä»¶

    print(f"è¯è¡¨å¤§å°: {len(tokenizer.vocab)}")

    # æ›´æ–° vocab_size ä¸ºå®é™…å¤§å°ï¼ˆå¯èƒ½ä¸å‚æ•°ä¸å®Œå…¨ä¸€è‡´ï¼‰
    args.vocab_size = len(tokenizer.vocab)

    # ==========================================
    # Step 5: åˆ›å»ºæ¨¡å‹
    # ==========================================
    print("\n" + "=" * 60)
    print("åˆ›å»ºæ¨¡å‹")
    print("=" * 60)

    # åˆ›å»ºæ¨¡å‹é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è¶…å‚æ•°
    model_config = GPTConfig(
        vocab_size=args.vocab_size,     # è¯è¡¨å¤§å°
        emb_dim=args.emb_dim,           # åµŒå…¥ç»´åº¦
        num_heads=args.num_heads,       # æ³¨æ„åŠ›å¤´æ•°
        num_layers=args.num_layers,     # Transformer å±‚æ•°
        context_size=args.context_size, # ä¸Šä¸‹æ–‡é•¿åº¦
        dropout=args.dropout            # Dropout æ¯”ä¾‹
    )

    # åˆ›å»ºæ¨¡å‹å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
    model = GPT(model_config).to(device)

    # æ‰“å°æ¨¡å‹å‚æ•°é‡ï¼ˆ:, æ·»åŠ åƒä½åˆ†éš”ç¬¦ï¼‰
    print(f"æ¨¡å‹å‚æ•°é‡: {model.get_num_params():,}")

    # ============================================================
    # Step 6: åŠ è½½å·²æœ‰æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœè·³è¿‡æŸäº›é˜¶æ®µï¼‰
    # ============================================================
    # å¦‚æœè·³è¿‡é¢„è®­ç»ƒï¼Œå°è¯•åŠ è½½å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹
    if args.skip_pretrain:
        pretrain_path = os.path.join(args.checkpoint_dir, "pretrain_final.pt")
        if os.path.exists(pretrain_path):
            print(f"åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrain_path}")
            # torch.load åŠ è½½æ¨¡å‹æƒé‡
            # map_location ç¡®ä¿åœ¨ä¸åŒè®¾å¤‡é—´å…¼å®¹
            # weights_only=True æé«˜å®‰å…¨æ€§ï¼ŒåªåŠ è½½æƒé‡
            model.load_state_dict(torch.load(pretrain_path, map_location=device, weights_only=True))

    # å¦‚æœè·³è¿‡ SFTï¼Œå°è¯•åŠ è½½å·²æœ‰çš„ SFT æ¨¡å‹
    if args.skip_sft:
        sft_path = os.path.join(args.checkpoint_dir, "sft_final.pt")
        if os.path.exists(sft_path):
            print(f"åŠ è½½ SFT æ¨¡å‹: {sft_path}")
            model.load_state_dict(torch.load(sft_path, map_location=device, weights_only=True))

    # ==========================================
    # Step 7: å¼€å§‹ 5 é˜¶æ®µè®­ç»ƒ
    # ==========================================
    # æŒ‰é¡ºåºæ‰§è¡Œå„ä¸ªè®­ç»ƒé˜¶æ®µï¼ˆå¯ä»¥é€šè¿‡ --skip-xxx è·³è¿‡ï¼‰

    # ------------------------------------------------------------
    # é˜¶æ®µ 1ï¼šé¢„è®­ç»ƒ (Pretrain)
    # ------------------------------------------------------------
    # ç›®çš„ï¼šä»å¤§é‡æ–‡æœ¬ä¸­å­¦ä¹ è¯­è¨€è§„å¾‹
    # ä»»åŠ¡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆè¯­è¨€å»ºæ¨¡ï¼‰
    if not args.skip_pretrain:
        train_pretrain(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡é¢„è®­ç»ƒé˜¶æ®µ")

    # ------------------------------------------------------------
    # é˜¶æ®µ 2ï¼šç›‘ç£å¾®è°ƒ (SFT)
    # ------------------------------------------------------------
    # ç›®çš„ï¼šå­¦ä¹ å¯¹è¯æ ¼å¼å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›
    # ä»»åŠ¡ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ç”ŸæˆåŠ©æ‰‹å›ç­”
    if not args.skip_sft:
        train_sft(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ SFT é˜¶æ®µ")

    # ------------------------------------------------------------
    # é˜¶æ®µ 3ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ (Reward Model)
    # ------------------------------------------------------------
    # ç›®çš„ï¼šå­¦ä¹ äººç±»åå¥½ï¼Œèƒ½å¤Ÿç»™å›ç­”æ‰“åˆ†
    # ä»»åŠ¡ï¼šåŒºåˆ†å¥½å›ç­”å’Œå·®å›ç­”
    reward_model = None  # åˆå§‹åŒ–ä¸º Noneï¼Œå¦‚æœè·³è¿‡æˆ–å¤±è´¥åˆ™ä¿æŒ None
    if not args.skip_reward:
        reward_model = train_reward_model(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
        # å¦‚æœè·³è¿‡è®­ç»ƒä½†éœ€è¦è¿›è¡Œ RLHFï¼Œå°è¯•åŠ è½½å·²æœ‰çš„å¥–åŠ±æ¨¡å‹
        reward_path = os.path.join(args.checkpoint_dir, "reward_model.pt")
        if os.path.exists(reward_path) and not args.skip_rlhf:
            # å»¶è¿Ÿå¯¼å…¥
            from reward_model import RewardModel
            # åˆ›å»ºå¥–åŠ±æ¨¡å‹é…ç½®
            model_config = MyLLMConfig(
                vocab_size=args.vocab_size,
                emb_dim=args.emb_dim,
                num_heads=args.num_heads,
                num_layers=args.num_layers,
                context_size=args.context_size
            )
            # åˆ›å»ºå¹¶åŠ è½½å¥–åŠ±æ¨¡å‹
            reward_model = RewardModel(model_config)
            reward_model.load_state_dict(torch.load(reward_path, map_location=device, weights_only=True))
            reward_model.to(device)  # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            print(f"åŠ è½½å·²æœ‰å¥–åŠ±æ¨¡å‹: {reward_path}")

    # ------------------------------------------------------------
    # é˜¶æ®µ 4ï¼šRLHF (åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )
    # ------------------------------------------------------------
    # ç›®çš„ï¼šåˆ©ç”¨å¥–åŠ±æ¨¡å‹ä¼˜åŒ–ç­–ç•¥ï¼Œç”Ÿæˆæ›´å¥½çš„å›ç­”
    # æ–¹æ³•ï¼šPPO ç®—æ³•
    if not args.skip_rlhf:
        train_rlhf(model, reward_model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ RLHF é˜¶æ®µ")

    # ------------------------------------------------------------
    # é˜¶æ®µ 5ï¼šRLVF (å¯éªŒè¯åé¦ˆå¼ºåŒ–å­¦ä¹ )
    # ------------------------------------------------------------
    # ç›®çš„ï¼šæå‡ç²¾ç¡®æ¨ç†èƒ½åŠ›ï¼ˆæ•°å­¦ã€é€»è¾‘é¢˜ï¼‰
    # æ–¹æ³•ï¼šä½¿ç”¨è‡ªåŠ¨éªŒè¯å™¨ä»£æ›¿å¥–åŠ±æ¨¡å‹
    if not args.skip_rlvf:
        train_rlvf(model, tokenizer, args, device)
    else:
        print("\nè·³è¿‡ RLVF é˜¶æ®µ")

    # ==========================================
    # Step 8: è®­ç»ƒå®Œæˆï¼Œæ‰“å°æ€»ç»“
    # ==========================================
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"\næ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: {args.checkpoint_dir}/")
    print("  - pretrain_final.pt  (é¢„è®­ç»ƒæ¨¡å‹)")
    print("  - sft_final.pt       (SFT æ¨¡å‹)")
    print("  - reward_model.pt    (å¥–åŠ±æ¨¡å‹)")
    print("  - rlhf_final.pt      (RLHF æ¨¡å‹)")
    print("  - rlvf_final.pt      (RLVF æ¨¡å‹)")


# ============================================================
# ç¨‹åºå…¥å£
# ============================================================
# Python çš„æ ‡å‡†å…¥å£ç‚¹æ¨¡å¼ï¼š
# å½“ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶ï¼ˆpython train.pyï¼‰ï¼Œ__name__ == "__main__"
# å½“è¢«å…¶ä»–æ–‡ä»¶ import æ—¶ï¼Œ__name__ == "train"
# è¿™æ ·å¯ä»¥é˜²æ­¢ import æ—¶æ„å¤–æ‰§è¡Œè®­ç»ƒ
if __name__ == "__main__":
    main()
