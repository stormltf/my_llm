"""
å¤§è¯­è¨€æ¨¡å‹æ ¸å¿ƒç»„ä»¶å®ç°

åŒ…å«ï¼š
1. LayerNorm: å±‚å½’ä¸€åŒ–
2. FeedForward: å‰é¦ˆç¥ç»ç½‘ç»œ
3. Attention: æ³¨æ„åŠ›æœºåˆ¶
4. MultiHeadAttention: å¤šå¤´æ³¨æ„åŠ›
5. TransformerBlock: Transformer å—
6. GPT: å®Œæ•´çš„ GPT æ¨¡å‹

å‚è€ƒï¼šGPT-2, LLaMA æ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from typing import Optional, Tuple


@dataclass
class GPTConfig:
    """
    GPT æ¨¡å‹é…ç½®

    è¿™é‡Œå®šä¹‰äº†æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œç›¸å½“äº"è®¾è®¡å›¾çº¸"
    """
    # æ¨¡å‹ç»“æ„å‚æ•°
    vocab_size: int = 1000          # è¯è¡¨å¤§å°ï¼ˆéœ€è¦å’Œ tokenizer åŒ¹é…ï¼‰
    emb_dim: int = 256              # è¯åµŒå…¥ç»´åº¦ï¼ˆæ¯ä¸ªtokenç”¨å¤šå°‘ç»´å‘é‡è¡¨ç¤ºï¼‰
    num_heads: int = 8              # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°ï¼ˆéœ€è¦èƒ½è¢« emb_dim æ•´é™¤ï¼‰
    num_layers: int = 6             # Transformer Block çš„å±‚æ•°
    context_size: int = 256         # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä¸€æ¬¡æœ€å¤šçœ‹å¤šå°‘ä¸ªtokenï¼‰

    # æ­£åˆ™åŒ–å‚æ•°
    dropout: float = 0.1            # Dropout æ¯”ä¾‹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    layer_norm_epsilon: float = 1e-5

    # å‰é¦ˆç½‘ç»œå‚æ•°
    ffn_multiplier: int = 4         # FFN ä¸­é—´å±‚ç»´åº¦æ”¾å¤§å€æ•°

    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        assert self.emb_dim % self.num_heads == 0, \
            f"emb_dim ({self.emb_dim}) å¿…é¡»èƒ½è¢« num_heads ({self.num_heads}) æ•´é™¤"


class LayerNorm(nn.Module):
    """
    å±‚å½’ä¸€åŒ– (Layer Normalization)

    ä½œç”¨ï¼šå°†æ¯ä¸€å±‚çš„è¾“å…¥å½’ä¸€åŒ–ï¼Œä½¿æ•°å€¼åˆ†å¸ƒæ›´ç¨³å®š
    å…¬å¼ï¼šLN(x) = scale * (x - mean) / sqrt(var + eps) + shift

    å¯¹æ¯” BatchNormï¼š
    - BatchNorm: å¯¹ batch ç»´åº¦åšå½’ä¸€åŒ–ï¼ˆCNN ä¸­å¸¸ç”¨ï¼‰
    - LayerNorm: å¯¹ç‰¹å¾ç»´åº¦åšå½’ä¸€åŒ–ï¼ˆNLP ä¸­å¸¸ç”¨ï¼Œå› ä¸ºåºåˆ—é•¿åº¦ä¸å›ºå®šï¼‰
    """

    def __init__(self, emb_dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        # å¯å­¦ä¹ å‚æ•°ï¼šç¼©æ”¾å’Œå¹³ç§»
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]

        Returns:
            å½’ä¸€åŒ–åçš„å¼ é‡
        """
        # è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šï¼‰
        mean = x.mean(dim=-1, keepdim=True)      # [batch, seq_len, 1]
        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [batch, seq_len, 1]

        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)

        # åº”ç”¨å¯å­¦ä¹ çš„ç¼©æ”¾å’Œå¹³ç§»
        return self.scale * x_norm + self.shift


class FeedForward(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œ (Feed-Forward Network)

    ç»“æ„ï¼šLinear -> GELU -> Linear
    ä½œç”¨ï¼šåœ¨ Attention ä¹‹åè¿›è¡Œ"æ€è€ƒ"å’Œ"è®°å¿†"

    ç»´åº¦å˜åŒ–ï¼šemb_dim -> emb_dim * multiplier -> emb_dim
    ä¾‹å¦‚ï¼š256 -> 1024 -> 256
    """

    def __init__(self, emb_dim: int, multiplier: int = 4, dropout: float = 0.1):
        super().__init__()
        # ç¬¬ä¸€å±‚ï¼šç»´åº¦æ”¾å¤§ï¼ˆå¢åŠ æ¨¡å‹å®¹é‡ï¼Œæ•æ‰æ›´å¤šæ¨¡å¼ï¼‰
        self.linear1 = nn.Linear(emb_dim, emb_dim * multiplier)
        # ç¬¬äºŒå±‚ï¼šç»´åº¦è¿˜åŸ
        self.linear2 = nn.Linear(emb_dim * multiplier, emb_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]

        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]
        """
        # ç»´åº¦æ”¾å¤§ + æ¿€æ´»å‡½æ•°
        x = self.linear1(x)
        x = F.gelu(x)  # GELU æ˜¯ç›®å‰ LLM æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°
        # ç»´åº¦è¿˜åŸ
        x = self.linear2(x)
        x = self.dropout(x)
        return x


class CausalSelfAttention(nn.Module):
    """
    å› æœè‡ªæ³¨æ„åŠ›æœºåˆ¶ (Causal Self-Attention)

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. æ¯ä¸ª token ä½œä¸º Query å»æŸ¥è¯¢å…¶ä»–æ‰€æœ‰ token
    2. é€šè¿‡ Q @ K è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰
    3. ç”¨æ³¨æ„åŠ›åˆ†æ•°å¯¹ Value è¿›è¡ŒåŠ æƒæ±‚å’Œ
    4. Causalï¼ˆå› æœï¼‰ï¼šå½“å‰ token åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ tokenï¼ˆç”¨ Mask å®ç°ï¼‰

    æ•°å­¦å…¬å¼ï¼š
    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.emb_dim = config.emb_dim
        self.num_heads = config.num_heads
        self.head_dim = config.emb_dim // config.num_heads  # æ¯ä¸ªå¤´è´Ÿè´£çš„ç»´åº¦
        self.context_size = config.context_size

        # ç¡®ä¿ emb_dim èƒ½è¢« num_heads æ•´é™¤
        assert self.head_dim * self.num_heads == self.emb_dim, \
            f"emb_dim ({config.emb_dim}) å¿…é¡»èƒ½è¢« num_heads ({config.num_heads}) æ•´é™¤"

        # Q, K, V çš„çº¿æ€§å˜æ¢å±‚
        # æ³¨æ„ï¼šè¿™é‡Œä¸åˆ†å¼€å®šä¹‰ä¸‰ä¸ªçŸ©é˜µï¼Œè€Œæ˜¯ä¸€ä¸ªå¤§çŸ©é˜µï¼Œæé«˜å¹¶è¡Œæ•ˆç‡
        self.c_attn = nn.Linear(config.emb_dim, 3 * config.emb_dim)  # è¾“å‡º Q, K, V æ‹¼æ¥

        # è¾“å‡ºæŠ•å½±å±‚
        self.c_proj = nn.Linear(config.emb_dim, config.emb_dim)

        # Dropout
        self.dropout = nn.Dropout(config.dropout)
        self.resid_dropout = nn.Dropout(config.dropout)

        # å› æœ Maskï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼‰
        # ä½œç”¨ï¼šè®©å½“å‰ä½ç½®åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ä½ç½®ï¼Œçœ‹ä¸åˆ°æœªæ¥çš„ä½ç½®
        # ä¾‹å¦‚ï¼šä½ç½® 2 å¯ä»¥çœ‹åˆ°ä½ç½® 0, 1, 2ï¼Œä½†çœ‹ä¸åˆ° 3, 4, 5...
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config.context_size, config.context_size))
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]

        Returns:
            æ³¨æ„åŠ›è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]
        """
        batch_size, seq_len, emb_dim = x.shape

        # 1. è®¡ç®— Q, K, V
        # qkv å½¢çŠ¶: [batch, seq_len, 3 * emb_dim]
        qkv = self.c_attn(x)

        # åˆ†å‰²æˆ Q, K, V
        # æ¯ä¸ªå½¢çŠ¶: [batch, seq_len, emb_dim]
        q, k, v = qkv.split(self.emb_dim, dim=2)

        # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
        # å½¢çŠ¶å˜æ¢: [batch, seq_len, num_heads, head_dim]
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim)

        # 3. è½¬ç½®ï¼Œæ–¹ä¾¿å¹¶è¡Œè®¡ç®—
        # å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # 4. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Q @ K^T)
        # å½¢çŠ¶: [batch, num_heads, seq_len, seq_len]
        # attn_score[i, h, j, k] è¡¨ç¤ºç¬¬ i ä¸ªæ ·æœ¬ã€ç¬¬ h ä¸ªå¤´ä¸­ï¼Œä½ç½® j å¯¹ä½ç½® k çš„æ³¨æ„åŠ›åˆ†æ•°
        attn_score = q @ k.transpose(-2, -1)

        # 5. ç¼©æ”¾ (é™¤ä»¥ sqrt(d_k))ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
        attn_score = attn_score / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))

        # 6. åº”ç”¨å› æœ Maskï¼ˆæŠŠæœªæ¥ä½ç½®çš„åˆ†æ•°è®¾ä¸º -infï¼‰
        # è¿™æ · softmax åè¿™äº›ä½ç½®çš„æƒé‡å°±ä¼šæ¥è¿‘ 0
        if seq_len > 1:  # æ¨ç†æ—¶ seq_len=1 ä¸éœ€è¦ mask
            # åªå–éœ€è¦çš„ mask éƒ¨åˆ†
            mask = self.mask[:seq_len, :seq_len]
            # å°† mask=0 çš„ä½ç½®è®¾ä¸º -inf
            attn_score = attn_score.masked_fill(mask == 0, float('-inf'))

        # 7. Softmax å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡
        # å½¢çŠ¶: [batch, num_heads, seq_len, seq_len]
        attn_weight = F.softmax(attn_score, dim=-1)
        attn_weight = self.dropout(attn_weight)

        # 8. åŠ æƒæ±‚å’Œ (æƒé‡ @ Value)
        # å½¢çŠ¶: [batch, num_heads, seq_len, head_dim]
        context_vec = attn_weight @ v

        # 9. åˆå¹¶å¤šå¤´
        # å…ˆè½¬å›: [batch, seq_len, num_heads, head_dim]
        context_vec = context_vec.transpose(1, 2)
        # å† reshape: [batch, seq_len, emb_dim]
        context_vec = context_vec.reshape(batch_size, seq_len, emb_dim)

        # 10. è¾“å‡ºæŠ•å½±
        output = self.c_proj(context_vec)
        output = self.resid_dropout(output)

        return output


class TransformerBlock(nn.Module):
    """
    Transformer Block (Transformer å—)

    è¿™æ˜¯ GPT çš„åŸºæœ¬æ„å»ºå•å…ƒï¼ŒåŒ…å«ï¼š
    1. Layer Norm -> Multi-Head Attention -> Residual
    2. Layer Norm -> Feed Forward -> Residual

    æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ Pre-Normï¼ˆNorm åœ¨å‰é¢ï¼‰ï¼Œè¿™æ˜¯ç›®å‰ä¸»æµåšæ³•ï¼ˆLLaMA, GPT-2, Qwen ç­‰ï¼‰
    ç›¸æ¯” Post-Normï¼ˆNorm åœ¨åé¢ï¼‰ï¼ŒPre-Norm è®­ç»ƒæ›´ç¨³å®šã€‚
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        # ç¬¬ä¸€ä¸ªå­å±‚ï¼šè‡ªæ³¨æ„åŠ›
        self.ln_1 = LayerNorm(config.emb_dim, eps=config.layer_norm_epsilon)
        self.attn = CausalSelfAttention(config)

        # ç¬¬äºŒä¸ªå­å±‚ï¼šå‰é¦ˆç½‘ç»œ
        self.ln_2 = LayerNorm(config.emb_dim, eps=config.layer_norm_epsilon)
        self.mlp = FeedForward(
            config.emb_dim,
            multiplier=config.ffn_multiplier,
            dropout=config.dropout
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]

        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, emb_dim]
        """
        # å­å±‚1ï¼šAttention + Residual
        # æ³¨æ„ï¼šResidual æ˜¯ x + layer_outputï¼Œä¿è¯æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±
        x = x + self.attn(self.ln_1(x))

        # å­å±‚2ï¼šFeedForward + Residual
        x = x + self.mlp(self.ln_2(x))

        return x


class GPT(nn.Module):
    """
    GPT (Generative Pre-trained Transformer) æ¨¡å‹

    å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹ç»“æ„ï¼š
    Token Embedding -> Position Embedding -> [Transformer Block] * N -> Layer Norm -> Output Projection
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config

        # 1. Token Embeddingï¼šå°† token ID æ˜ å°„ä¸ºå‘é‡
        # å½¢çŠ¶: [vocab_size, emb_dim]
        self.tok_emb = nn.Embedding(config.vocab_size, config.emb_dim)

        # 2. Position Embeddingï¼šä¸ºæ¯ä¸ªä½ç½®åˆ†é…ä¸€ä¸ªå‘é‡
        # å½¢çŠ¶: [context_size, emb_dim]
        self.pos_emb = nn.Embedding(config.context_size, config.emb_dim)

        # Dropout
        self.drop = nn.Dropout(config.dropout)

        # 3. Transformer Blocks å †å 
        # è¿™æ˜¯æ¨¡å‹çš„"å¤§è„‘"éƒ¨åˆ†
        self.blocks = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.num_layers)
        ])

        # 4. æœ€åçš„ Layer Norm
        self.ln_f = LayerNorm(config.emb_dim, eps=config.layer_norm_epsilon)

        # 5. è¾“å‡ºæŠ•å½±ï¼šå°†å‘é‡æ˜ å°„å›è¯è¡¨
        # å½¢çŠ¶: [emb_dim, vocab_size]
        self.lm_head = nn.Linear(config.emb_dim, config.vocab_size, bias=False)

        # æƒé‡å…±äº«ï¼šToken Embedding å’Œè¾“å‡ºå±‚å…±äº«æƒé‡ï¼ˆå¯ä»¥å‡å°‘å‚æ•°é‡ï¼‰
        # è¿™æ˜¯ GPT-2 å¼€å§‹çš„åšæ³•
        self.tok_emb.weight = self.lm_head.weight

        # åˆå§‹åŒ–æƒé‡ï¼ˆé‡è¦ï¼å¥½çš„åˆå§‹åŒ–èƒ½è®©è®­ç»ƒæ›´ç¨³å®šï¼‰
        self.apply(self._init_weights)

        # å…¼å®¹æ€§åˆ«åï¼ˆä¾› reward_model.py ä½¿ç”¨ï¼‰
        self.token_embedding = self.tok_emb
        self.position_embedding = self.pos_emb
        self.transformer_blocks = self.blocks
        self.final_norm = self.ln_f

        print(f"GPT æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°é‡: {self.get_num_params():,}")

    def _init_weights(self, module):
        """
        æƒé‡åˆå§‹åŒ–

        å‚è€ƒ GPT-2 çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼š
        - Linear: æ­£æ€åˆ†å¸ƒï¼Œstd=0.02
        - Embedding: æ­£æ€åˆ†å¸ƒï¼Œstd=0.02
        - LayerNorm: scale=1, shift=0
        """
        if isinstance(module, nn.Linear):
            # ç‰¹æ®Šå¤„ç†æœ€åä¸€å±‚ lm_headï¼Œè®©è¾“å‡ºæ–¹å·®å°ä¸€äº›
            if module is self.lm_head:
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            else:
                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
                if module.bias is not None:
                    torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, LayerNorm):
            torch.nn.init.ones_(module.scale)
            torch.nn.init.zeros_(module.shift)

    def get_num_params(self):
        """è¿”å›æ¨¡å‹å‚æ•°é‡"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)

    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­

        Args:
            idx: è¾“å…¥ token ID åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len]
            targets: ç›®æ ‡ token ID åºåˆ—ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰ï¼Œå½¢çŠ¶ [batch_size, seq_len]

        Returns:
            logits: æ¨¡å‹è¾“å‡ºï¼Œå½¢çŠ¶ [batch_size, seq_len, vocab_size]
            loss: æŸå¤±å€¼ï¼ˆåªæœ‰è®­ç»ƒæ—¶æ‰è®¡ç®—ï¼‰
        """
        device = idx.device
        batch_size, seq_len = idx.shape

        # ç¡®ä¿ seq_len ä¸è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        assert seq_len <= self.config.context_size, \
            f"åºåˆ—é•¿åº¦ {seq_len} è¶…è¿‡æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ {self.config.context_size}"

        # 1. è·å– token embeddings
        # å½¢çŠ¶: [batch_size, seq_len, emb_dim]
        tok_emb = self.tok_emb(idx)

        # 2. è·å– position embeddings
        # ç”Ÿæˆä½ç½®ç´¢å¼•: [0, 1, 2, ..., seq_len-1]
        pos = torch.arange(0, seq_len, dtype=torch.long, device=device)
        # å½¢çŠ¶: [seq_len, emb_dim] -> [batch_size, seq_len, emb_dim]
        pos_emb = self.pos_emb(pos).unsqueeze(0)

        # 3. åˆå¹¶ token å’Œ position embeddings
        # å½¢çŠ¶: [batch_size, seq_len, emb_dim]
        x = self.drop(tok_emb + pos_emb)

        # 4. é€šè¿‡ Transformer Blocks
        for block in self.blocks:
            x = block(x)

        # 5. æœ€åçš„ Layer Norm
        x = self.ln_f(x)

        # 6. è¾“å‡ºæŠ•å½±
        # å½¢çŠ¶: [batch_size, seq_len, vocab_size]
        logits = self.lm_head(x)

        # 7. è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›äº† targetsï¼‰
        loss = None
        if targets is not None:
            # å°† logits å±•å¹³ä¸º [batch_size * seq_len, vocab_size]
            # å°† targets å±•å¹³ä¸º [batch_size * seq_len]
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1),
                ignore_index=-1  # å¿½ç•¥ padding ä½ç½®çš„ loss
            )

        return logits, loss

    def num_parameters(self) -> int:
        """è¿”å›æ¨¡å‹å‚æ•°é‡ï¼ˆå…¼å®¹æ¥å£ï¼‰"""
        return self.get_num_params()

    @torch.no_grad()
    def generate(
        self,
        idx: torch.Tensor,
        max_new_tokens: int = 50,
        temperature: float = 1.0,
        top_k: int = 0,
        top_p: float = 1.0,
        eos_token_id: int = None
    ) -> torch.Tensor:
        """
        è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ

        Args:
            idx: è¾“å…¥ token ID åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len]
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§
            top_k: åªä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­é‡‡æ ·
            top_p: åªä»ç´¯ç§¯æ¦‚ç‡è¾¾åˆ° p çš„ token ä¸­é‡‡æ ·
            eos_token_id: ç»“æŸ token IDï¼Œé‡åˆ°æ—¶åœæ­¢ç”Ÿæˆ

        Returns:
            ç”Ÿæˆçš„å®Œæ•´åºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len + new_tokens]
        """
        self.eval()

        for _ in range(max_new_tokens):
            # æˆªæ–­è¾“å…¥ï¼Œç¡®ä¿ä¸è¶…è¿‡ä¸Šä¸‹æ–‡é•¿åº¦
            idx_cond = idx if idx.size(1) <= self.config.context_size else idx[:, -self.config.context_size:]

            # å‰å‘ä¼ æ’­
            logits, _ = self(idx_cond)

            # å–æœ€åä¸€ä¸ªä½ç½®çš„ logits
            logits = logits[:, -1, :] / temperature

            # Top-k é‡‡æ ·
            if top_k > 0:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = float('-inf')

            # Top-p (nucleus) é‡‡æ ·
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

                # ç§»é™¤ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ token
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
                sorted_indices_to_remove[:, 0] = 0

                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                logits[indices_to_remove] = float('-inf')

            # é‡‡æ ·
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)

            # æ‹¼æ¥åˆ°åºåˆ—
            idx = torch.cat((idx, idx_next), dim=1)

            # æ£€æŸ¥æ˜¯å¦é‡åˆ°ç»“æŸ token
            if eos_token_id is not None and (idx_next == eos_token_id).all():
                break

        return idx


# ==========================================
# å…¼å®¹æ€§åˆ«å
# ==========================================
# ä¸ºäº†å…¼å®¹ reward_model.py, rlhf.py, rlvf.py ä¸­çš„å¯¼å…¥
MyLLM = GPT


def demo_model():
    """
    æ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
    """
    print("=" * 60)
    print("GPT æ¨¡å‹ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = GPTConfig(
        vocab_size=1000,
        emb_dim=256,
        num_heads=8,
        num_layers=6,
        context_size=256,
        dropout=0.1
    )

    # åˆ›å»ºæ¨¡å‹
    model = GPT(config)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    seq_len = 10
    idx = torch.randint(0, config.vocab_size, (batch_size, seq_len))

    print(f"\nè¾“å…¥å½¢çŠ¶: {idx.shape}")

    # å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—lossï¼‰
    logits, loss = model(idx)
    print(f"è¾“å‡ºå½¢çŠ¶: {logits.shape}")
    print(f"Loss: {loss}")

    # å‰å‘ä¼ æ’­ï¼ˆè®¡ç®—lossï¼‰
    targets = torch.randint(0, config.vocab_size, (batch_size, seq_len))
    logits, loss = model(idx, targets)
    print(f"\nå¸¦è®­ç»ƒçš„è¾“å‡ºå½¢çŠ¶: {logits.shape}")
    print(f"è®­ç»ƒLoss: {loss.item():.4f}")

    print("\n" + "=" * 60)


def demo_forward_step_by_step():
    """
    ğŸ“ å‰å‘ä¼ æ’­è¯¦ç»†æ¼”ç¤º

    ç”¨ä¸€ä¸ªè¶…å°æ¨¡å‹ï¼Œé€æ­¥å±•ç¤ºæ•°æ®å¦‚ä½•æµè¿‡ç½‘ç»œçš„æ¯ä¸€å±‚ã€‚
    è¿™ä¸ªæ¼”ç¤ºä½¿ç”¨å…·ä½“æ•°å­—ï¼Œè®©ä½ ç›´è§‚ç†è§£å‰å‘ä¼ æ’­çš„å…¨è¿‡ç¨‹ã€‚
    """
    print("\n" + "=" * 70)
    print("ğŸ“ å‰å‘ä¼ æ’­è¯¦ç»†æ¼”ç¤º - ç”¨å…·ä½“æ•°å­—ç†è§£æ¯ä¸€æ­¥")
    print("=" * 70)

    # ================================================================
    # é…ç½®ä¸€ä¸ªè¶…å°æ¨¡å‹ï¼Œä¾¿äºè§‚å¯Ÿ
    # ================================================================
    print("\nğŸ“‹ ã€ç¬¬0æ­¥ï¼šæ¨¡å‹é…ç½®ã€‘")
    print("-" * 50)

    config = GPTConfig(
        vocab_size=5,      # åªæœ‰5ä¸ªè¯ï¼šå‡è®¾æ˜¯ ["æˆ‘", "çˆ±", "AI", "å­¦", "ä¹ "]
        emb_dim=4,         # æ¯ä¸ªè¯ç”¨4ä¸ªæ•°å­—è¡¨ç¤º
        num_heads=2,       # 2ä¸ªæ³¨æ„åŠ›å¤´
        num_layers=1,      # åªç”¨1å±‚Transformerï¼ˆä¾¿äºè§‚å¯Ÿï¼‰
        context_size=8,    # æœ€å¤§çœ‹8ä¸ªè¯
        dropout=0.0,       # å…³é—­dropoutï¼Œç»“æœå¯å¤ç°
    )

    print(f"   è¯è¡¨å¤§å° (vocab_size):  {config.vocab_size} ä¸ªè¯")
    print(f"   åµŒå…¥ç»´åº¦ (emb_dim):     {config.emb_dim} ç»´å‘é‡")
    print(f"   æ³¨æ„åŠ›å¤´æ•° (num_heads): {config.num_heads} ä¸ªå¤´")
    print(f"   æ¯ä¸ªå¤´çš„ç»´åº¦:           {config.emb_dim // config.num_heads} ç»´")
    print(f"   Transformerå±‚æ•°:        {config.num_layers} å±‚")

    # è®¾ç½®éšæœºç§å­ï¼Œè®©ç»“æœå¯å¤ç°
    torch.manual_seed(42)

    # åˆ›å»ºæ¨¡å‹ï¼ˆä¸æ‰“å°å‚æ•°é‡ä¿¡æ¯ï¼‰
    import sys
    from io import StringIO
    old_stdout = sys.stdout
    sys.stdout = StringIO()
    model = GPT(config)
    sys.stdout = old_stdout
    model.eval()  # è¯„ä¼°æ¨¡å¼

    # ================================================================
    # å‡†å¤‡è¾“å…¥
    # ================================================================
    print("\nğŸ“¥ ã€ç¬¬1æ­¥ï¼šå‡†å¤‡è¾“å…¥ã€‘")
    print("-" * 50)

    # å‡è®¾è¾“å…¥æ˜¯ "æˆ‘ çˆ±" -> token ID = [0, 1]
    idx = torch.tensor([[0, 1]])  # å½¢çŠ¶: [1, 2] (1ä¸ªå¥å­ï¼Œ2ä¸ªè¯)

    print(f"   å‡è®¾è¯è¡¨: ['æˆ‘', 'çˆ±', 'AI', 'å­¦', 'ä¹ ']")
    print(f"            ID:  0     1     2     3     4")
    print(f"")
    print(f"   è¾“å…¥å¥å­: 'æˆ‘ çˆ±'")
    print(f"   Token IDs: {idx.tolist()[0]}")
    print(f"   å¼ é‡å½¢çŠ¶: {list(idx.shape)} = [batch_size=1, seq_len=2]")

    # ================================================================
    # ç¬¬2æ­¥ï¼šToken Embedding
    # ================================================================
    print("\nğŸ“Š ã€ç¬¬2æ­¥ï¼šToken Embedding - æŠŠIDå˜æˆå‘é‡ã€‘")
    print("-" * 50)

    tok_emb = model.tok_emb(idx)

    print(f"   æ“ä½œ: tok_emb = model.tok_emb(idx)")
    print(f"   åŸç†: ç”¨ token ID å»åµŒå…¥è¡¨é‡ŒæŸ¥å¯¹åº”çš„è¡Œ")
    print(f"")
    print(f"   åµŒå…¥è¡¨ (vocab_size Ã— emb_dim = 5 Ã— 4):")
    emb_weight = model.tok_emb.weight.data
    for i in range(config.vocab_size):
        word = ['æˆ‘', 'çˆ±', 'AI', 'å­¦', 'ä¹ '][i]
        vec = emb_weight[i].tolist()
        vec_str = [f"{v:+.3f}" for v in vec]
        print(f"      ID={i} '{word}' â†’ [{', '.join(vec_str)}]")

    print(f"")
    print(f"   æŸ¥è¡¨è¿‡ç¨‹:")
    print(f"      ID=0 'æˆ‘' â†’ å–ç¬¬0è¡Œ")
    print(f"      ID=1 'çˆ±' â†’ å–ç¬¬1è¡Œ")
    print(f"")
    print(f"   tok_emb ç»“æœ (å½¢çŠ¶ {list(tok_emb.shape)} = [batch, seq_len, emb_dim]):")
    for i, word in enumerate(['æˆ‘', 'çˆ±']):
        vec = tok_emb[0, i].tolist()
        vec_str = [f"{v:+.3f}" for v in vec]
        print(f"      '{word}' â†’ [{', '.join(vec_str)}]")

    # ================================================================
    # ç¬¬3æ­¥ï¼šPosition Embedding
    # ================================================================
    print("\nğŸ“ ã€ç¬¬3æ­¥ï¼šPosition Embedding - åŠ ä¸Šä½ç½®ä¿¡æ¯ã€‘")
    print("-" * 50)

    seq_len = idx.shape[1]
    pos = torch.arange(0, seq_len, dtype=torch.long)
    pos_emb = model.pos_emb(pos)

    print(f"   æ“ä½œ: pos_emb = model.pos_emb([0, 1])")
    print(f"   åŸç†: æ¯ä¸ªä½ç½®æœ‰ä¸€ä¸ªå¯¹åº”çš„å‘é‡ï¼Œå‘Šè¯‰æ¨¡å‹è¯çš„é¡ºåº")
    print(f"")
    print(f"   ä½ç½®åµŒå…¥è¡¨ (context_size Ã— emb_dim = 8 Ã— 4) å‰2è¡Œ:")
    pos_weight = model.pos_emb.weight.data
    for i in range(2):
        vec = pos_weight[i].tolist()
        vec_str = [f"{v:+.3f}" for v in vec]
        print(f"      ä½ç½®{i} â†’ [{', '.join(vec_str)}]")

    print(f"")
    print(f"   pos_emb ç»“æœ (å½¢çŠ¶ {list(pos_emb.shape)}):")
    for i in range(2):
        vec = pos_emb[i].tolist()
        vec_str = [f"{v:+.3f}" for v in vec]
        print(f"      ä½ç½®{i} â†’ [{', '.join(vec_str)}]")

    # ================================================================
    # ç¬¬4æ­¥ï¼šåˆå¹¶åµŒå…¥
    # ================================================================
    print("\nâ• ã€ç¬¬4æ­¥ï¼šåˆå¹¶åµŒå…¥ - Tokenå‘é‡ + ä½ç½®å‘é‡ã€‘")
    print("-" * 50)

    x = tok_emb + pos_emb.unsqueeze(0)

    print(f"   æ“ä½œ: x = tok_emb + pos_emb")
    print(f"   åŸç†: é€å…ƒç´ ç›¸åŠ ï¼Œè®©æ¯ä¸ªè¯åŒæ—¶çŸ¥é“'è‡ªå·±æ˜¯è°'å’Œ'åœ¨å“ªä¸ªä½ç½®'")
    print(f"")
    print(f"   è®¡ç®—è¿‡ç¨‹:")
    for i, word in enumerate(['æˆ‘', 'çˆ±']):
        tok_vec = tok_emb[0, i].tolist()
        pos_vec = pos_emb[i].tolist()
        result = x[0, i].tolist()
        print(f"      '{word}' (ä½ç½®{i}):")
        print(f"         Token:    [{', '.join([f'{v:+.3f}' for v in tok_vec])}]")
        print(f"       + Position: [{', '.join([f'{v:+.3f}' for v in pos_vec])}]")
        print(f"       = åˆå¹¶ç»“æœ: [{', '.join([f'{v:+.3f}' for v in result])}]")

    print(f"")
    print(f"   x å½¢çŠ¶: {list(x.shape)} = [batch=1, seq_len=2, emb_dim=4]")

    # ================================================================
    # ç¬¬5æ­¥ï¼šTransformer Block
    # ================================================================
    print("\nğŸ”„ ã€ç¬¬5æ­¥ï¼šTransformer Block - è¯ä¹‹é—´äº¤æµã€‘")
    print("-" * 50)
    print(f"   æ¯ä¸ªBlockåŒ…å«ä¸¤ä¸ªå­å±‚:")
    print(f"   1. Self-Attention: è¯ä¸è¯ä¹‹é—´äº¤æµä¿¡æ¯")
    print(f"   2. Feed-Forward:   æ¯ä¸ªè¯ç‹¬ç«‹è¿›è¡Œæ·±åº¦å¤„ç†")

    # è¿›å…¥ç¬¬ä¸€ä¸ªblock
    block = model.blocks[0]

    # 5.1 LayerNorm + Attention
    print(f"\n   ğŸ“Œ 5.1 Self-Attention å­å±‚")
    print(f"   " + "-" * 40)

    ln_out = block.ln_1(x)
    print(f"   a) LayerNorm: æ ‡å‡†åŒ–æ•°å€¼")
    print(f"      è¾“å…¥ x:     å‡å€¼={x.mean().item():.3f}, æ ‡å‡†å·®={x.std().item():.3f}")
    print(f"      è¾“å‡º ln_x:  å‡å€¼={ln_out.mean().item():.3f}, æ ‡å‡†å·®={ln_out.std().item():.3f}")

    # è®¡ç®—Q, K, V
    attn = block.attn
    qkv = attn.c_attn(ln_out)
    q, k, v = qkv.split(config.emb_dim, dim=2)

    print(f"\n   b) ç”Ÿæˆ Q, K, V:")
    print(f"      Q (Query - æˆ‘è¦æ‰¾ä»€ä¹ˆ):  å½¢çŠ¶ {list(q.shape)}")
    print(f"      K (Key - æˆ‘æ˜¯ä»€ä¹ˆ):      å½¢çŠ¶ {list(k.shape)}")
    print(f"      V (Value - æˆ‘çš„å†…å®¹):    å½¢çŠ¶ {list(v.shape)}")

    # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
    batch_size = 1
    q = q.view(batch_size, seq_len, config.num_heads, config.emb_dim // config.num_heads).transpose(1, 2)
    k = k.view(batch_size, seq_len, config.num_heads, config.emb_dim // config.num_heads).transpose(1, 2)
    v = v.view(batch_size, seq_len, config.num_heads, config.emb_dim // config.num_heads).transpose(1, 2)

    print(f"\n   c) é‡å¡‘ä¸ºå¤šå¤´å½¢å¼ (2ä¸ªå¤´ï¼Œæ¯å¤´2ç»´):")
    print(f"      Q å½¢çŠ¶: {list(q.shape)} = [batch, num_heads, seq_len, head_dim]")

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    head_dim = config.emb_dim // config.num_heads
    attn_score = q @ k.transpose(-2, -1) / (head_dim ** 0.5)

    print(f"\n   d) è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (Q @ K^T / âˆšd):")
    print(f"      æ³¨æ„åŠ›åˆ†æ•°å½¢çŠ¶: {list(attn_score.shape)} = [batch, heads, seq, seq]")
    print(f"      ")
    print(f"      æ¯ä¸ªå€¼è¡¨ç¤º: 'è¡Œä½ç½®' å¯¹ 'åˆ—ä½ç½®' çš„å…³æ³¨ç¨‹åº¦")
    print(f"      ")
    for h in range(config.num_heads):
        print(f"      å¤´{h}çš„æ³¨æ„åŠ›åˆ†æ•°:")
        print(f"              'æˆ‘'     'çˆ±'")
        for i, word in enumerate(['æˆ‘', 'çˆ±']):
            scores = attn_score[0, h, i].tolist()
            print(f"         '{word}':  {scores[0]:+.3f}   {scores[1]:+.3f}")

    # åº”ç”¨å› æœMask
    mask = torch.tril(torch.ones(seq_len, seq_len))
    attn_score_masked = attn_score.masked_fill(mask == 0, float('-inf'))

    print(f"\n   e) åº”ç”¨å› æœMask (å½“å‰è¯åªèƒ½çœ‹ä¹‹å‰çš„è¯):")
    print(f"      ")
    print(f"      MaskçŸ©é˜µ:     åº”ç”¨å:")
    print(f"        'æˆ‘' 'çˆ±'     ")
    print(f"      [[1,   0],      'æˆ‘'åªèƒ½çœ‹è‡ªå·±")
    print(f"       [1,   1]]      'çˆ±'å¯ä»¥çœ‹'æˆ‘'å’Œè‡ªå·±")
    print(f"      ")
    print(f"      å¤´0 maskå:")
    print(f"              'æˆ‘'     'çˆ±'")
    for i, word in enumerate(['æˆ‘', 'çˆ±']):
        scores = attn_score_masked[0, 0, i].tolist()
        s0 = f"{scores[0]:+.3f}" if scores[0] != float('-inf') else "  -âˆ  "
        s1 = f"{scores[1]:+.3f}" if scores[1] != float('-inf') else "  -âˆ  "
        print(f"         '{word}':  {s0}   {s1}")

    # Softmax
    attn_weight = F.softmax(attn_score_masked, dim=-1)

    print(f"\n   f) Softmaxå½’ä¸€åŒ– (å˜æˆæ¦‚ç‡ï¼Œæ¯è¡ŒåŠ èµ·æ¥=1):")
    print(f"      ")
    print(f"      å¤´0çš„æ³¨æ„åŠ›æƒé‡:")
    print(f"              'æˆ‘'     'çˆ±'      (æ¯è¡Œå’Œ=1)")
    for i, word in enumerate(['æˆ‘', 'çˆ±']):
        weights = attn_weight[0, 0, i].tolist()
        row_sum = sum(weights)
        print(f"         '{word}':  {weights[0]:.3f}    {weights[1]:.3f}     ({row_sum:.3f})")

    print(f"\n      ğŸ’¡ è§£è¯»:")
    print(f"         'æˆ‘' â†’ 100%å…³æ³¨è‡ªå·± (å› ä¸ºçœ‹ä¸åˆ°'çˆ±')")
    print(f"         'çˆ±' â†’ éƒ¨åˆ†å…³æ³¨'æˆ‘'ï¼Œéƒ¨åˆ†å…³æ³¨è‡ªå·±")

    # åŠ æƒæ±‚å’Œ
    context = attn_weight @ v

    print(f"\n   g) ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒæ±‚å’Œ Value:")
    print(f"      context = attn_weight @ V")
    print(f"      å½¢çŠ¶: {list(context.shape)}")
    print(f"")
    print(f"      'æˆ‘'çš„è¾“å‡º = 1.0 Ã— V['æˆ‘'] + 0.0 Ã— V['çˆ±'] = V['æˆ‘']")
    print(f"      'çˆ±'çš„è¾“å‡º = w1 Ã— V['æˆ‘'] + w2 Ã— V['çˆ±']  (åŠ æƒæ··åˆ)")

    # æ®‹å·®è¿æ¥
    attn_out = block.attn(block.ln_1(x))
    x_after_attn = x + attn_out

    print(f"\n   h) æ®‹å·®è¿æ¥: x = x + attention_output")
    print(f"      åŸç†: ä¿ç•™åŸå§‹ä¿¡æ¯ï¼Œé˜²æ­¢æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±")

    # 5.2 Feed-Forward
    print(f"\n   ğŸ“Œ 5.2 Feed-Forward å­å±‚")
    print(f"   " + "-" * 40)

    ln2_out = block.ln_2(x_after_attn)
    ffn_out = block.mlp(ln2_out)
    x_after_ffn = x_after_attn + ffn_out

    print(f"   a) LayerNorm")
    print(f"   b) Feed-Forward: æ‰©å¤§â†’æ¿€æ´»â†’ç¼©å°")
    print(f"      è¾“å…¥ç»´åº¦: {config.emb_dim}")
    print(f"      ä¸­é—´ç»´åº¦: {config.emb_dim * 4} (æ‰©å¤§4å€)")
    print(f"      è¾“å‡ºç»´åº¦: {config.emb_dim}")
    print(f"   c) æ®‹å·®è¿æ¥")
    print(f"")
    print(f"   Blockè¾“å‡ºå½¢çŠ¶: {list(x_after_ffn.shape)}")

    # ================================================================
    # ç¬¬6æ­¥ï¼šæœ€åçš„LayerNorm
    # ================================================================
    print("\nğŸ“ ã€ç¬¬6æ­¥ï¼šæœ€åçš„LayerNormã€‘")
    print("-" * 50)

    # é€šè¿‡æ‰€æœ‰blocks
    x_final = x
    for block in model.blocks:
        x_final = block(x_final)
    x_normed = model.ln_f(x_final)

    print(f"   æ“ä½œ: x = LayerNorm(x)")
    print(f"   åŸç†: æœ€åå†æ ‡å‡†åŒ–ä¸€æ¬¡ï¼Œç¡®ä¿è¾“å‡ºç¨³å®š")
    print(f"   è¾“å‡ºå½¢çŠ¶: {list(x_normed.shape)}")

    # ================================================================
    # ç¬¬7æ­¥ï¼šè¾“å‡ºæŠ•å½±
    # ================================================================
    print("\nğŸ¯ ã€ç¬¬7æ­¥ï¼šè¾“å‡ºæŠ•å½± - å‘é‡å˜æˆè¯è¡¨æ¦‚ç‡ã€‘")
    print("-" * 50)

    logits = model.lm_head(x_normed)

    print(f"   æ“ä½œ: logits = model.lm_head(x)")
    print(f"   åŸç†: æŠŠ{config.emb_dim}ç»´å‘é‡æ˜ å°„åˆ°{config.vocab_size}ç»´ï¼ˆè¯è¡¨å¤§å°ï¼‰")
    print(f"")
    print(f"   æƒé‡çŸ©é˜µå½¢çŠ¶: [{config.emb_dim}, {config.vocab_size}]")
    print(f"   logits å½¢çŠ¶:  {list(logits.shape)} = [batch, seq_len, vocab_size]")
    print(f"")
    print(f"   æ¯ä¸ªä½ç½®çš„è¾“å‡º (5ä¸ªè¯çš„åˆ†æ•°):")
    for i, word in enumerate(['æˆ‘', 'çˆ±']):
        scores = logits[0, i].tolist()
        print(f"      ä½ç½®{i} '{word}' é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„åˆ†æ•°:")
        for j, w in enumerate(['æˆ‘', 'çˆ±', 'AI', 'å­¦', 'ä¹ ']):
            print(f"         '{w}': {scores[j]:+.3f}")

    # ================================================================
    # ç¬¬8æ­¥ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯
    # ================================================================
    print("\nğŸ”® ã€ç¬¬8æ­¥ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‘")
    print("-" * 50)

    # å–æœ€åä¸€ä¸ªä½ç½®çš„logits
    last_logits = logits[0, -1, :]  # å½¢çŠ¶: [vocab_size]

    # Softmaxå¾—åˆ°æ¦‚ç‡
    probs = F.softmax(last_logits, dim=-1)

    print(f"   è¾“å…¥: 'æˆ‘ çˆ±'")
    print(f"   ä»»åŠ¡: é¢„æµ‹ 'çˆ±' åé¢çš„è¯")
    print(f"")
    print(f"   å–æœ€åä½ç½®çš„logitsï¼Œç”¨Softmaxå˜æˆæ¦‚ç‡:")
    print(f"")
    words = ['æˆ‘', 'çˆ±', 'AI', 'å­¦', 'ä¹ ']
    for j, w in enumerate(words):
        print(f"      '{w}': logit={last_logits[j]:+.3f} â†’ æ¦‚ç‡={probs[j]:.1%}")

    # é¢„æµ‹
    pred_id = probs.argmax().item()
    pred_word = words[pred_id]

    print(f"")
    print(f"   âœ… é¢„æµ‹ç»“æœ: '{pred_word}' (æ¦‚ç‡æœ€é«˜)")
    print(f"   å®Œæ•´è¾“å‡º: 'æˆ‘ çˆ± {pred_word}'")

    # ================================================================
    # æ€»ç»“
    # ================================================================
    print("\n" + "=" * 70)
    print("ğŸ“š å‰å‘ä¼ æ’­æ€»ç»“")
    print("=" * 70)
    print("""
    è¾“å…¥ "æˆ‘ çˆ±" [0, 1]                    å½¢çŠ¶å˜åŒ–
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 1. Token Embedding              â”‚   [1,2] â†’ [1,2,4]
    â”‚    ID â†’ å‘é‡ (æŸ¥è¡¨)              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 2. Position Embedding           â”‚   + [2,4]
    â”‚    åŠ ä¸Šä½ç½®ä¿¡æ¯                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 3. Transformer Block            â”‚   [1,2,4] â†’ [1,2,4]
    â”‚    - Self-Attention: è¯é—´äº¤æµ   â”‚
    â”‚    - Feed-Forward: æ·±åº¦å¤„ç†     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 4. LayerNorm                    â”‚   [1,2,4] â†’ [1,2,4]
    â”‚    æ ‡å‡†åŒ–è¾“å‡º                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ 5. è¾“å‡ºæŠ•å½± (lm_head)           â”‚   [1,2,4] â†’ [1,2,5]
    â”‚    å‘é‡ â†’ è¯è¡¨æ¦‚ç‡              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
    Softmax â†’ é¢„æµ‹ "AI" (æ¦‚ç‡æœ€é«˜çš„è¯)
    """)
    print("=" * 70)


if __name__ == "__main__":
    demo_model()
    print("\n" + "ğŸ“" * 30 + "\n")
    demo_forward_step_by_step()
