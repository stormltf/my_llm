# MyLLM - 从零手搓大模型

一个完整的大语言模型学习项目，从零实现 GPT 风格的语言模型，包含完整的 5 阶段训练流程。

## 项目简介

本项目旨在帮助开发者理解大语言模型的底层原理，通过亲手实现每个组件，打破对大模型的神秘感。

### 核心特性

- **完整的 Transformer 架构**：从头实现注意力机制、前馈网络等核心组件
- **字符级分词器**：简单易懂的中文分词实现
- **5 阶段训练流程**：Pretrain → SFT → Reward Model → RLHF → RLVF
- **CPU 可训练**：~3.7M 参数的迷你模型，普通电脑即可运行

---

## 整体架构概览

### 大模型训练全景图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           大模型训练完整流程                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        第一部分：数据准备                             │   │
│  │  ┌───────────┐                                                      │   │
│  │  │ 原始文本   │ ──► Tokenizer ──► Token IDs ──► 训练数据            │   │
│  │  └───────────┘     (分词器)       [15,23,5...]                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        第二部分：模型架构                             │   │
│  │                                                                     │   │
│  │  Token IDs ──► Embedding ──► Transformer Blocks ──► Output Layer   │   │
│  │                                    ↓                                │   │
│  │                         [Self-Attention + FFN] × N                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    ↓                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        第三部分：训练流程                             │   │
│  │                                                                     │   │
│  │   Stage 1        Stage 2        Stage 3        Stage 4    Stage 5  │   │
│  │  ┌────────┐    ┌────────┐    ┌────────┐    ┌────────┐  ┌────────┐ │   │
│  │  │Pretrain│ ─► │  SFT   │ ─► │ Reward │ ─► │  RLHF  │─►│  RLVF  │ │   │
│  │  │预训练  │    │监督微调│    │  Model │    │  PPO   │  │可验证RL│ │   │
│  │  └────────┘    └────────┘    └────────┘    └────────┘  └────────┘ │   │
│  │      │              │             │             │           │      │   │
│  │      ▼              ▼             ▼             ▼           ▼      │   │
│  │   语言建模      对话能力       偏好学习      策略优化     精确推理  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 各阶段目标

| 阶段 | 名称 | 目标 | 数据类型 |
|------|------|------|----------|
| 1 | Pretrain | 学习语言规律，预测下一个词 | 大量无标注文本 |
| 2 | SFT | 学习对话格式，获得指令遵循能力 | 人工标注的对话数据 |
| 3 | Reward Model | 学习人类偏好，区分好坏回答 | 偏好对比数据 |
| 4 | RLHF | 生成符合人类偏好的回答 | 奖励模型 + PPO |
| 5 | RLVF | 提升精确推理能力 | 可验证的任务 |

---

## 项目结构

```
my_llm/
├── tokenizer.py            # [阶段0] 分词器
├── model.py                # [模型] Transformer 架构
├── config.py               # [配置] 模型和训练参数
├── train.py                # [训练] 完整 5 阶段流程
├── reward_model.py         # [阶段3] 奖励模型
├── rlhf.py                 # [阶段4] RLHF (PPO) 训练器
├── rlvf.py                 # [阶段5] RLVF 训练器
├── inference.py            # [推理] 文本生成
├── data/
│   ├── pretrain_data.txt   # 预训练文本
│   ├── sft_data.json       # SFT 对话数据 (94条)
│   ├── reward_data.json    # 奖励数据 (40对)
│   └── rlvf_data.json      # RLVF 任务 (40条)
└── checkpoints/            # 模型检查点
```

---

## 环境要求

- **Python**: 3.8+
- **操作系统**: Linux / macOS / Windows
- **硬件**: CPU 即可运行（GPU 可加速）

---

## 安装步骤

```bash
# 1. 克隆项目
git clone https://github.com/your-repo/my_llm.git
cd my_llm

# 2. 创建虚拟环境（推荐）
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# 或 venv\Scripts\activate  # Windows

# 3. 安装依赖
pip install -r requirements.txt

# 4. 验证环境（可选但推荐）
python3 verify_setup.py
```

---

## 快速开始

```bash
# 运行完整训练流程
python3 train.py

# 跳过特定阶段
python3 train.py --skip-pretrain --skip-sft
```

---

## 各脚本启动命令

### 1. 环境验证

```bash
# 检查环境配置和数据文件是否完整
python3 verify_setup.py
```

### 2. 模型训练 (train.py)

```bash
# 基本训练（默认参数）
python3 train.py

# 自定义训练参数
python3 train.py --epochs 100 --batch_size 32 --lr 1e-3

# 从检查点恢复训练
python3 train.py --resume --checkpoint_path checkpoints/checkpoint_epoch_50.pt

# 常用参数说明：
#   --epochs        训练轮数（默认 10）
#   --batch_size    批次大小（默认 32）
#   --lr            学习率（默认 1e-3）
#   --seq_len       序列长度（默认 64）
#   --save_every    保存频率（默认每 10 轮）
```

### 3. 文本生成 (generate.py)

```bash
# 指定提示词生成文本
python3 generate.py --prompt "你好" --max_length 50

# 进入交互式对话模式
python3 generate.py --interactive

# 调整生成参数
python3 generate.py --prompt "人工智能" \
    --temperature 0.8 \
    --top_k 10 \
    --top_p 0.9 \
    --max_length 100

# 贪婪解码（确定性输出）
python3 generate.py --prompt "你好" --greedy

# 使用指定模型
python3 generate.py --checkpoint checkpoints/sft_final.pt --prompt "你好"
```

### 4. 模型验证 (test_model.py)

```bash
# 验证默认模型
python3 test_model.py

# 显示详细输出
python3 test_model.py --verbose

# 验证指定模型
python3 test_model.py --model checkpoints/sft_final.pt
```

---

## 阶段 0：分词器（Tokenizer）

**文件**：`tokenizer.py`

分词器是 LLM 的"翻译官"，将人类文本转换为模型可处理的数字序列。

### 为什么需要分词

```
人类语言: "你好，世界"  ←── 模型无法直接理解
    ↓ 分词器
数字序列: [15, 23, 5, 89, 102]  ←── 模型可以处理
```

### 工作原理

```
┌─────────────────────────────────────────────────────────────┐
│                     分词器工作流程                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   输入: "你好，世界"                                         │
│         ↓                                                   │
│   Step 1: 字符切分                                          │
│         ["你", "好", "，", "世", "界"]                       │
│         ↓                                                   │
│   Step 2: 查词表 (Vocabulary)                               │
│         ┌──────────────────────┐                           │
│         │  词表 (训练时构建)    │                           │
│         │  ──────────────────  │                           │
│         │  <PAD> → 0           │                           │
│         │  <UNK> → 1           │                           │
│         │  你    → 15          │                           │
│         │  好    → 23          │                           │
│         │  ，    → 5           │                           │
│         │  世    → 89          │                           │
│         │  界    → 102         │                           │
│         └──────────────────────┘                           │
│         ↓                                                   │
│   输出: [15, 23, 5, 89, 102]                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 特殊 Token

| Token | ID | 用途 | 示例 |
|-------|-----|------|------|
| `<PAD>` | 0 | 填充序列到相同长度 | 短序列补齐 |
| `<UNK>` | 1 | 表示未知字符 | 生僻字 |
| `<BOS>` | 2 | 标记序列开始 | 文本起始 |
| `<EOS>` | 3 | 标记序列结束 | 文本终止 |
| `<\|im_start\|>` | 4 | 对话角色开始 | 区分 user/assistant |
| `<\|im_end\|>` | 5 | 对话角色结束 | 角色发言结束 |

### 对话格式（ChatML）

```
<|im_start|>user
你好，请介绍一下自己。<|im_end|>
<|im_start|>assistant
你好！我是 MyLLM，一个小型语言模型。<|im_end|>
```

### 代码示例

```python
from tokenizer import MyLLMTokenizer

tokenizer = MyLLMTokenizer()
tokenizer.build_vocab("训练文本...")  # 构建词表

ids = tokenizer.encode("你好")        # 编码: [15, 23]
text = tokenizer.decode([15, 23])     # 解码: "你好"

tokenizer.save("vocab.json")          # 保存词表
tokenizer.load("vocab.json")          # 加载词表
```

---

## 模型架构：Transformer

**文件**：`model.py`

Transformer 是现代 LLM 的核心架构，本项目实现了 GPT 风格的 Decoder-Only 结构。

### 整体结构

```
┌─────────────────────────────────────────────────────────────┐
│                    Transformer 架构                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   输入: Token IDs [15, 23, 5]                               │
│         ↓                                                   │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Token Embedding                                    │   │
│   │  将每个 ID 映射为 256 维向量                          │   │
│   │  [15,23,5] → [[0.1,0.2,...], [0.3,0.4,...], ...]   │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓ +                                                 │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Position Embedding                                 │   │
│   │  添加位置信息（第1个词、第2个词...）                    │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓                                                   │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Transformer Block × 4 层                           │   │
│   │  ┌─────────────────────────────────────────────┐   │   │
│   │  │  LayerNorm                                  │   │   │
│   │  │       ↓                                     │   │   │
│   │  │  Multi-Head Self-Attention (4 heads)       │   │   │
│   │  │  "让每个词看看其他词，决定关注谁"              │   │   │
│   │  │       ↓ + 残差连接                          │   │   │
│   │  │  LayerNorm                                  │   │   │
│   │  │       ↓                                     │   │   │
│   │  │  Feed-Forward Network                       │   │   │
│   │  │  "深度处理，提取高级特征"                     │   │   │
│   │  │       ↓ + 残差连接                          │   │   │
│   │  └─────────────────────────────────────────────┘   │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓                                                   │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Output Projection                                  │   │
│   │  [batch, seq, 256] → [batch, seq, vocab_size]      │   │
│   │  输出每个位置预测下一个词的概率分布                    │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓                                                   │
│   输出: 下一个 token 的概率分布                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### Self-Attention 详解

Self-Attention 是 Transformer 的核心，让模型学会"关注"输入中的相关部分。

```
┌─────────────────────────────────────────────────────────────┐
│                   Self-Attention 机制                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   输入 X: "我 爱 学习"                                       │
│         ↓                                                   │
│   生成 Q, K, V:                                             │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Q (Query) = X × W_q   "我想找什么信息？"            │   │
│   │  K (Key)   = X × W_k   "我有什么标签？"              │   │
│   │  V (Value) = X × W_v   "我的实际内容是什么？"         │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓                                                   │
│   计算注意力分数:                                            │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Attention = softmax(Q × K^T / √d) × V              │   │
│   │                                                     │   │
│   │  例如计算"学习"对其他词的注意力:                       │   │
│   │       我    爱    学习                               │   │
│   │      0.1   0.3   0.6   ← 注意力权重 (归一化后)        │   │
│   │                                                     │   │
│   │  "学习"更关注自己(0.6)和"爱"(0.3)                    │   │
│   └─────────────────────────────────────────────────────┘   │
│         ↓                                                   │
│   因果遮罩 (Causal Mask):                                   │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  GPT 是自回归模型，只能看到之前的词                    │   │
│   │                                                     │   │
│   │       我    爱    学习                               │   │
│   │  我   [✓]  [✗]   [✗]                                │   │
│   │  爱   [✓]  [✓]   [✗]                                │   │
│   │  学习 [✓]  [✓]   [✓]                                │   │
│   │                                                     │   │
│   │  ✓ = 可以看到, ✗ = 被遮罩                            │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 模型配置

```python
from config import get_mini_config

config = get_mini_config()
# vocab_size:    6400    词表大小
# emb_dim:       256     嵌入维度
# num_heads:     4       注意力头数
# num_layers:    4       Transformer 层数
# context_size:  256     最大上下文长度
# 参数量:        ~3.7M
```

---

## 阶段 1：预训练（Pretrain）

**目标**：学习语言的基本规律，能够预测下一个词。

### 训练方式

```
┌─────────────────────────────────────────────────────────────┐
│                      预训练过程                              │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   训练数据: "人工智能是计算机科学的一个分支"                   │
│                                                             │
│   自回归训练 (预测下一个词):                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  输入: [人]           → 预测: 工                     │   │
│   │  输入: [人,工]        → 预测: 智                     │   │
│   │  输入: [人,工,智]     → 预测: 能                     │   │
│   │  输入: [人,工,智,能]  → 预测: 是                     │   │
│   │  ...                                                │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   损失函数: Cross-Entropy Loss                              │
│   L = -log(P(正确的下一个词))                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 代码实现

```python
# train.py 中的预训练逻辑
for epoch in range(num_epochs):
    for input_ids, target_ids in dataloader:
        # 前向传播
        logits, loss = model(input_ids, target_ids)

        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### 预训练后的能力

- ✅ 理解语言结构和语法
- ✅ 学习词语之间的关联
- ✅ 能够续写文本
- ❌ 不会遵循指令
- ❌ 不会进行对话

---

## 阶段 2：SFT（监督微调）

**目标**：学习对话格式，获得指令遵循能力。

### 训练方式

```
┌─────────────────────────────────────────────────────────────┐
│                       SFT 训练过程                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   训练数据格式:                                              │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  <|im_start|>user                                   │   │
│   │  什么是机器学习？<|im_end|>                          │   │
│   │  <|im_start|>assistant                              │   │
│   │  机器学习是人工智能的一个分支，让计算机从数据中         │   │
│   │  学习规律，而不需要明确编程。<|im_end|>               │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   训练目标:                                                  │
│   - 只计算 assistant 回复部分的 loss                        │
│   - 学习"看到用户问题后，生成合适的回复"                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### SFT 数据示例

```json
{
  "user": "什么是机器学习？",
  "assistant": "机器学习是人工智能的一个分支，让计算机从数据中学习规律..."
}
```

### SFT 后的能力

- ✅ 预训练的所有能力
- ✅ 理解对话格式
- ✅ 遵循用户指令
- ✅ 生成有帮助的回复
- ❌ 可能生成不安全/不准确的内容
- ❌ 不一定符合人类偏好

---

## 阶段 3：奖励模型（Reward Model）

**文件**：`reward_model.py`

**目标**：学习人类偏好，能够给回答打分。

### 工作原理

```
┌─────────────────────────────────────────────────────────────┐
│                     奖励模型训练                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   训练数据 (偏好对):                                         │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Prompt: "什么是机器学习？"                          │   │
│   │                                                     │   │
│   │  Chosen (人类偏好):                                  │   │
│   │  "机器学习是AI的分支，让计算机从数据中学习规律，       │   │
│   │   包括监督学习、无监督学习等方法。"                   │   │
│   │                                                     │   │
│   │  Rejected (人类不偏好):                              │   │
│   │  "机器学习就是机器在学习，很简单的概念。"             │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   模型结构:                                                  │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  输入: Prompt + Response                            │   │
│   │      ↓                                              │   │
│   │  Transformer Encoder (复用预训练权重)                │   │
│   │      ↓                                              │   │
│   │  Reward Head (线性层)                               │   │
│   │      ↓                                              │   │
│   │  输出: 标量奖励分数 r ∈ ℝ                           │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   损失函数 (Bradley-Terry):                                  │
│   L = -log(σ(r_chosen - r_rejected))                        │
│                                                             │
│   目标: 让 r_chosen > r_rejected                            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 代码示例

```python
from reward_model import RewardModel, RewardModelTrainer

# 从预训练模型初始化
reward_model = RewardModel.from_pretrained(base_model, config)

# 训练
trainer = RewardModelTrainer(reward_model, tokenizer, config)
trainer.train(reward_data, epochs=3)

# 使用: 给回答打分
score = reward_model(prompt + response)  # 返回标量分数
```

---

## 阶段 4：RLHF（基于人类反馈的强化学习）

**文件**：`rlhf.py`

**目标**：利用奖励模型指导策略优化，生成更符合人类偏好的回答。

### PPO 算法流程

```
┌─────────────────────────────────────────────────────────────┐
│                      RLHF (PPO) 训练                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   每个训练步骤:                                              │
│                                                             │
│   Step 1: 采样                                              │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Prompt: "如何学习编程？"                            │   │
│   │      ↓                                              │   │
│   │  Policy Model (当前策略) 生成回答                    │   │
│   │      ↓                                              │   │
│   │  Response: "学习编程需要..."                         │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   Step 2: 评分                                              │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Reward Model 对回答打分                             │   │
│   │  reward = RM(prompt, response) = 0.85               │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   Step 3: 计算优势                                          │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Advantage = reward - baseline                      │   │
│   │  (正优势 = 比平均好，负优势 = 比平均差)               │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   Step 4: PPO 更新                                          │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  概率比: r(θ) = π_new(a|s) / π_old(a|s)            │   │
│   │                                                     │   │
│   │  PPO 损失 (带裁剪):                                  │   │
│   │  L = -min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A)          │   │
│   │                                                     │   │
│   │  裁剪作用: 防止策略更新过大导致训练不稳定             │   │
│   │                                                     │   │
│   │  KL 惩罚: 保持与原始策略接近                         │   │
│   │  L_total = L_ppo + β · KL(π_new || π_ref)          │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 关键参数

| 参数 | 说明 | 默认值 | 作用 |
|------|------|--------|------|
| `clip_ratio` | PPO 裁剪系数 | 0.2 | 限制策略更新幅度 |
| `kl_coef` | KL 惩罚系数 | 0.01 | 保持与原策略接近 |
| `num_episodes` | 训练轮数 | 50 | 采样和更新次数 |

### 代码示例

```python
from rlhf import PPOTrainer, RLHFConfig

rlhf_config = RLHFConfig(
    clip_ratio=0.2,
    kl_coef=0.01,
    num_episodes=50
)

trainer = PPOTrainer(
    policy_model=model,
    reward_model=reward_model,
    tokenizer=tokenizer,
    config=config,
    rlhf_config=rlhf_config
)

trainer.train(prompts)
```

### RLHF 后的能力

- ✅ 之前所有能力
- ✅ 生成更符合人类偏好的回答
- ✅ 更安全、更有帮助
- ❌ 对于有明确正确答案的问题，仍可能出错

---

## 阶段 5：RLVF（基于可验证反馈的强化学习）

**文件**：`rlvf.py`

**目标**：利用可自动验证的正确答案作为奖励信号，提升精确推理能力。

### 与 RLHF 的区别

```
┌─────────────────────────────────────────────────────────────┐
│                    RLHF vs RLVF 对比                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   RLHF:                                                     │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Prompt → Model → Response → Reward Model → 分数    │   │
│   │                              (主观偏好)              │   │
│   │  适用: 开放式问题                                    │   │
│   │  缺点: 需要人类标注，成本高                          │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
│   RLVF:                                                     │
│   ┌─────────────────────────────────────────────────────┐   │
│   │  Task → Model → Response → Verifier → 对/错         │   │
│   │                            (客观验证)               │   │
│   │  适用: 有明确答案的问题                              │   │
│   │  优点: 自动验证，无需人类标注                        │   │
│   └─────────────────────────────────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 验证器类型

**1. 数学验证器 (MathVerifier)**

```python
prompt:   "计算 15 + 27 = ?"
expected: "42"

response: "答案是 42。"     → reward = +1.0 (正确)
response: "答案是 43。"     → reward = -0.5 (错误)
```

**2. 逻辑验证器 (LogicVerifier)**

```python
prompt:   "所有猫都是动物，小花是猫，小花是动物吗？"
expected: "是"
keywords: ["是", "正确", "对"]

response: "是的，小花是动物。"  → reward = +1.0
response: "不一定。"           → reward = -0.5
```

### 训练流程

```
┌─────────────────────────────────────────────────────────────┐
│                      RLVF 训练流程                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   For each iteration:                                       │
│                                                             │
│   ┌─────────┐     ┌─────────┐     ┌─────────┐              │
│   │ 采样任务 │ ──► │ 模型生成 │ ──► │ 验证器   │              │
│   │         │     │ 回答    │     │ 判断对错 │              │
│   └─────────┘     └─────────┘     └─────────┘              │
│        │                               │                    │
│        │                               ▼                    │
│        │                        ┌─────────────┐            │
│        │                        │ 计算奖励     │            │
│        │                        │ 对: +1.0    │            │
│        │                        │ 错: -0.5    │            │
│        │                        └─────────────┘            │
│        │                               │                    │
│        │                               ▼                    │
│        │                        ┌─────────────┐            │
│        │                        │ PPO 更新    │            │
│        └────────────────────────│ 策略模型    │            │
│                                 └─────────────┘            │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 任务数据格式

```json
{
    "type": "math",
    "prompt": "计算 15 + 27 = ?",
    "expected_answer": "42"
}
```

```json
{
    "type": "logic",
    "prompt": "如果今天是周一，明天是周几？",
    "expected_answer": "周二",
    "keywords": ["周二", "星期二"]
}
```

### 代码示例

```python
from rlvf import RLVFTrainer, RLVFConfig

rlvf_config = RLVFConfig(
    correct_reward=1.0,
    incorrect_reward=-0.5,
    num_iterations=30
)

trainer = RLVFTrainer(model, tokenizer, config, rlvf_config)
trainer.train(tasks, batch_size=4)

# 评估
results = trainer.evaluate(test_tasks)
print(f"准确率: {results['accuracy']:.2%}")
```

---

## 训练数据统计

| 数据类型 | 文件 | 数量 | 用途 |
|----------|------|------|------|
| SFT 对话 | `sft_data.json` | 94 条 | 对话能力训练 |
| 偏好数据 | `reward_data.json` | 40 对 | 奖励模型训练 |
| RLVF 任务 | `rlvf_data.json` | 40 条 | 精确推理训练 |

---

## 完整训练命令

```bash
# 完整 5 阶段训练
python train.py

# 只训练基础阶段 (Pretrain + SFT)
python train.py --skip-rlhf --skip-rlvf

# 只训练 RLHF/RLVF (需要已有 SFT 模型)
python train.py --skip-pretrain --skip-sft

# 训练后的模型文件
checkpoints/
├── pretrain_final.pt   # 预训练模型
├── sft_final.pt        # SFT 模型
├── reward_model.pt     # 奖励模型
├── rlhf_final.pt       # RLHF 模型
└── rlvf_final.pt       # RLVF 模型
```

---

## 参考资料

- 《Build a Large Language Model (From Scratch)》
- "Attention Is All You Need" - Transformer 原论文
- "Training language models to follow instructions with human feedback" - InstructGPT/RLHF
- "Proximal Policy Optimization Algorithms" - PPO 论文
- "Constitutional AI" - Anthropic

---

## 许可证

MIT License

---

**祝学习愉快！**
